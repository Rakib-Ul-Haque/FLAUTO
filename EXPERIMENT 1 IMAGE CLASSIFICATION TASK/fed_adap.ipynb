{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486d31b9-8684-4d0a-b2f5-0e00d41e8ad1",
   "metadata": {},
   "source": [
    "<h4> This code is part of FLAUTO. It implements FedAdap. Date: 01/09/2025 </h4>\n",
    "<h4> Contact: rakibul.haque@utsa.edu </h4>  \n",
    "<h4> Cite as: R. U. Haque and P. Markopoulos,\"Federated Learning with Automated Dual-Level Hyperparameter Tuning\", 2025 <h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a26882b",
   "metadata": {
    "id": "7a26882b"
   },
   "source": [
    "<H1>Import Libraries</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473cd34e",
   "metadata": {
    "id": "473cd34e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from torchvision.datasets import CIFAR10\n",
    "from collections import Counter\n",
    "import random\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from torch import Tensor\n",
    "from typing import Type\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set font family for plots\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b21b0f-99de-4780-9213-7a44331d8043",
   "metadata": {},
   "source": [
    "<H1>Load Dataset</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ce049",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "738ce049",
    "outputId": "16e7254f-7275-464e-e7cb-552f9fb6ce34"
   },
   "outputs": [],
   "source": [
    "subset_dataset = torch.load('val_dataset.pth',weights_only=False)\n",
    "remaining_dataset = torch.load('test_dataset.pth',weights_only=False)\n",
    "\n",
    "# Create DataLoaders\n",
    "val_loader = DataLoader(subset_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(remaining_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40fd883",
   "metadata": {
    "id": "f40fd883"
   },
   "source": [
    "<H1>IID</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7a1568",
   "metadata": {
    "id": "fc7a1568"
   },
   "outputs": [],
   "source": [
    "def distribute_dataset_equally(dataset, num_clients):\n",
    "    # Group data by class\n",
    "    class_data = {}\n",
    "    for data, label in dataset:\n",
    "        if label not in class_data:\n",
    "            class_data[label] = []\n",
    "        class_data[label].append((data, label))\n",
    "\n",
    "    # Distribute data\n",
    "    client_data = [[] for _ in range(num_clients)]\n",
    "    for label, data in class_data.items():\n",
    "        data_len = len(data)\n",
    "        base_size = data_len // num_clients\n",
    "        remain = data_len - base_size * num_clients\n",
    "\n",
    "        current_idx = 0\n",
    "        for i in range(num_clients):\n",
    "            end_idx = current_idx + base_size + (1 if i < remain else 0)\n",
    "            client_data[i].extend(data[current_idx:end_idx])\n",
    "            current_idx = end_idx\n",
    "\n",
    "\n",
    "    print_iid_distribution(client_data)\n",
    "    plot_iid_dataset(client_data,num_clients)\n",
    "\n",
    "    return client_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cb27b0",
   "metadata": {
    "id": "46cb27b0"
   },
   "outputs": [],
   "source": [
    "#print client iid_data distribution\n",
    "def print_iid_distribution(iid_datasets):\n",
    "    # Check if the distribution is correct\n",
    "    for i, client_data in enumerate(iid_datasets):\n",
    "        print(f\"Client {i + 1} data size: {len(client_data)}\")\n",
    "        class_counts = {j: 0 for j in range(10)}\n",
    "        for _, label in client_data:\n",
    "            class_counts[label] += 1\n",
    "        print(f\"Class distribution: {class_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d896660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot client iid_data distribution\n",
    "def plot_iid_dataset(iid_datasets, num_clients):\n",
    "    # CIFAR-10 class names\n",
    "    class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "    # Calculate fraction of distribution for each class across clients\n",
    "    fractions = []\n",
    "    for client_data in iid_datasets:\n",
    "        class_counts = {j: 0 for j in range(10)}\n",
    "        for _, label in client_data:\n",
    "            class_counts[label] += 1\n",
    "        total_data = len(client_data)\n",
    "        fractions.append([class_counts[i] / total_data for i in range(10)])\n",
    "    fractions = np.array(fractions)\n",
    "    # Define colors for each class\n",
    "    colors = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"#FFA500\", \"#FF00FF\", \"#808080\", \"#00FF00\"]\n",
    "    # Generate the 3D bar chart\n",
    "    fig = plt.figure(figsize=(10, 7))  # Adjust the size here\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    xpos, ypos = np.meshgrid(np.arange(num_clients), np.arange(10), indexing=\"ij\")\n",
    "    for i in range(10):\n",
    "        ax.bar3d(xpos[:, i], ypos[:, i], np.zeros_like(xpos[:, i]),0.75, 0.75, fractions[:, i],shade=True, color=colors[i])\n",
    "\n",
    "    ax.set_xlabel('Client Number',labelpad=5)\n",
    "    ax.set_ylabel('CIFAR-10 Classes',labelpad=15)\n",
    "    ax.set_zlabel('Fraction of Distribution',labelpad=5)\n",
    "    ax.set_xticks(np.arange(0.5, num_clients))\n",
    "    ax.set_xticklabels([str(i+1) for i in range(num_clients)], rotation=45)\n",
    "    ax.set_yticks(np.arange(0.5, 10))\n",
    "    ax.set_yticklabels(class_names, rotation=-60)\n",
    "\n",
    "    #ax.view_init(elev=40, azim=60)\n",
    "    ax.view_init(elev=40, azim=10)\n",
    "\n",
    "    plt.subplots_adjust(left=0.01, right=0.99, bottom=0.01, top=0.99)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef90546",
   "metadata": {
    "id": "bef90546"
   },
   "source": [
    "<H1>non-IID</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7fa1c5",
   "metadata": {
    "id": "1f7fa1c5"
   },
   "outputs": [],
   "source": [
    "#Print the distribution non IID\n",
    "def print_distribution(client_data):\n",
    "    # Check distribution\n",
    "    for l, client_data_value in enumerate(client_data):\n",
    "        print(f\"Client {l + 1} data size: {len(client_data_value)}\")\n",
    "        class_counts = {j: 0 for j in range(10)}\n",
    "        for _, label in client_data_value:\n",
    "            class_counts[label] += 1\n",
    "        print(f\"Class distribution: {class_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8419b107",
   "metadata": {
    "id": "8419b107"
   },
   "outputs": [],
   "source": [
    "# #Non IID code\n",
    "def distribute_dataset_dirichlet(dataset, num_clients, alpha):\n",
    "    # Group data by class\n",
    "    class_data = {}\n",
    "    for data, label in dataset:\n",
    "        if label not in class_data:\n",
    "            class_data[label] = []\n",
    "        class_data[label].append((data, label))\n",
    "\n",
    "    client_data = [[] for _ in range(num_clients)]\n",
    "    for _, data_list in class_data.items():\n",
    "        # Shuffle data for randomness\n",
    "        np.random.shuffle(data_list)\n",
    "\n",
    "        # Get proportions for data split based on Dirichlet distribution\n",
    "        proportions = np.random.dirichlet([alpha]*num_clients)\n",
    "        # print(\"Proportions: \", proportions)\n",
    "        total_data = len(data_list)\n",
    "        # print(\"total_data: \", total_data)\n",
    "        data_splits = [int(proportions[i]*total_data) for i in range(num_clients)]\n",
    "        # print(\"Data_Split: \", data_splits)\n",
    "\n",
    "        # Adjust the splits to account for rounding errors\n",
    "        # print(\"Before data_split:\",data_splits)\n",
    "        data_splits[-1] += total_data - sum(data_splits)\n",
    "        print(\"After data_split:\",data_splits)\n",
    "\n",
    "        start_idx = 0\n",
    "        for i, split in enumerate(data_splits):\n",
    "            end_idx = start_idx + split\n",
    "            client_data[i].extend(data_list[start_idx:end_idx])\n",
    "            start_idx = end_idx\n",
    "    print(\"Client Number: \",num_clients, \"Alpha: \", alpha)\n",
    "    print_distribution(client_data)\n",
    "    plot_distribution(client_data, num_clients )\n",
    "    return client_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ebf4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the graphs for nonIID\n",
    "def plot_distribution(client_datasets, num_clients):\n",
    "    # CIFAR-10 class names\n",
    "    class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "    # Calculate fraction of distribution for each class across clients\n",
    "    fractions = []\n",
    "    for client_data in client_datasets:\n",
    "        class_counts = {j: 0 for j in range(10)}\n",
    "        for _, label in client_data:\n",
    "            class_counts[label] += 1\n",
    "        total_data = len(client_data)\n",
    "        # print(total_data)\n",
    "        fractions.append([class_counts[i] / total_data for i in range(10)])\n",
    "    fractions = np.array(fractions)\n",
    "    # Define distinct colors for classes using a colormap\n",
    "    colors = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"#FFA500\", \"#FF00FF\", \"#808080\", \"#00FF00\"]\n",
    "\n",
    "    # Generate the 3D bar chart\n",
    "    fig = plt.figure(figsize=(10, 7))  # Adjust the size here\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    xpos, ypos = np.meshgrid(np.arange(num_clients), np.arange(10), indexing=\"ij\")\n",
    "\n",
    "    for i in range(10):\n",
    "        ax.bar3d(xpos[:, i], ypos[:, i], np.zeros_like(xpos[:, i]),0.75, 0.75, fractions[:, i],shade=True, color=colors[i])\n",
    "\n",
    "    ax.set_xlabel('Client Number')\n",
    "    ax.set_ylabel('CIFAR-10 Classes')\n",
    "    ax.set_zlabel('Fraction of Distribution')\n",
    "    ax.set_title('Distribution of CIFAR-10 Classes across Clients based on Dirichlet Distribution')\n",
    "    ax.set_xticks(np.arange(0.5, num_clients))\n",
    "    ax.set_xticklabels([str(i+1) for i in range(num_clients)], fontsize=12)\n",
    "    ax.set_yticks(np.arange(0.5, 10))\n",
    "    ax.set_yticklabels(class_names)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f95a6",
   "metadata": {},
   "source": [
    "# Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2700fb3",
   "metadata": {
    "id": "f2700fb3"
   },
   "outputs": [],
   "source": [
    "def accuracy(outp, target):\n",
    "    \"\"\"Computes accuracy\"\"\"\n",
    "    with torch.no_grad():\n",
    "        pred = torch.argmax(outp, dim=1)\n",
    "        correct = pred.eq(target).float().sum().item()\n",
    "        return 100.0 * correct / target.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9306ada",
   "metadata": {
    "id": "b9306ada"
   },
   "outputs": [],
   "source": [
    "def Print(string, dictionary):\n",
    "    first_key = next(iter(dictionary))\n",
    "    first_value = dictionary[first_key]\n",
    "    print(f\"{string}:{first_key}: {first_value[0][0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db37362",
   "metadata": {
    "id": "0db37362"
   },
   "outputs": [],
   "source": [
    "def forbinus_norm_function(w_i):\n",
    "    value = 0\n",
    "    for k in w_i.keys():\n",
    "        value += torch.linalg.norm(w_i[k])\n",
    "    return value.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e387f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_deviation_function(w_i, w_f):\n",
    "    model_deviation = 0\n",
    "    for k in w_i.keys():\n",
    "        model_deviation += torch.linalg.norm(w_f[k].to(torch.float) - w_i[k].to(torch.float)) / torch.linalg.norm(w_i[k].to(torch.float))\n",
    "    #print(model_deviation.item())\n",
    "    return model_deviation.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53678293",
   "metadata": {
    "id": "53678293"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a3ffbe",
   "metadata": {
    "id": "e0a3ffbe"
   },
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 10)  # Change output to 10 for CIFAR-10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.relu4(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu5(self.fc1(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7eaf8a",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d43d8",
   "metadata": {
    "id": "833d43d8"
   },
   "outputs": [],
   "source": [
    "def train(i_weights, epochs, train_loader, le_rate, cli,roun, epoch_flag):\n",
    "    global opti\n",
    "\n",
    "    e_a, e_l=test(i_weights, test_loader)\n",
    "    test_loss_epoch.append(e_l)\n",
    "    \n",
    "    local_model = model().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if opti==\"adam\":\n",
    "        optimizer = torch.optim.Adam(local_model.parameters(), lr=le_rate)\n",
    "    elif opti==\"sgd\":\n",
    "        optimizer = torch.optim.SGD(local_model.parameters(), lr=le_rate)\n",
    "    \n",
    "    epoch_train_accuracy=0 \n",
    "    epoch_train_loss=0\n",
    "    epoch_test_accuracy=0\n",
    "    epoch_test_loss=0\n",
    "    epoch_rmd=0\n",
    "\n",
    "    local_model.load_state_dict(i_weights)\n",
    "\n",
    "    local_model.train()  # Set the model to training mode\n",
    "\n",
    "    flag_counter=0\n",
    "    lr=le_rate\n",
    "    # initial weights cathing and printing\n",
    "    initial_weights = {k: v.clone() for k, v in local_model.state_dict().items()}\n",
    "    #Print(\"Model's inside the function Initial weights for client\",initial_weights)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_flag=epoch_flag+1\n",
    "        # gradients_this_epoch = {}\n",
    "        total_samples = 0\n",
    "        total_loss=0\n",
    "        correct_samples = 0\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward + backward + optimize\n",
    "            outputs = local_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = outputs.max(1)  # Get the index of the maximum value in outputs (predicted class)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_samples += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        if(total_samples!=0 and len(train_loader)!=0):\n",
    "            epoch_accuracy = 100 * correct_samples / total_samples\n",
    "            epoch_loss = total_loss / len(train_loader)\n",
    "        else:\n",
    "            epoch_accuracy = 100 * correct_samples / (total_samples+1)\n",
    "            epoch_loss = total_loss / (len(train_loader)+1)\n",
    "        print(f\"Round {roun}, cleint {cli+1}, epoch {epoch+1}, current LR: {optimizer.param_groups[0]['lr']}, epoch_accuracy {epoch_accuracy}, epoch_loss {epoch_loss} \")\n",
    "\n",
    "        a_f_w = {k: v.clone() for k, v in local_model.state_dict().items()}\n",
    "        \n",
    "        e_a, e_l=test(a_f_w, test_loader)\n",
    "        test_loss_epoch.append(e_l)\n",
    "        \n",
    "        print(f\"test loss difference {test_loss_epoch[epoch] - test_loss_epoch[epoch-1]}\")\n",
    "        \n",
    "        if abs(test_loss_epoch[epoch] - test_loss_epoch[epoch-1]) < 0.002:\n",
    "            lr= lr * 0.5\n",
    "            optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "            current_learning_rate = optimizer.param_groups[0]['lr']\n",
    "            #print(\"Current learning rate:\", current_learning_rate)\n",
    "\n",
    "            flag_counter=flag_counter+1\n",
    "\n",
    "            if flag_counter==5:\n",
    "                break\n",
    "\n",
    "\n",
    "    \n",
    "    f_weights = {k: v.clone() for k, v in local_model.state_dict().items()}\n",
    "\n",
    "    #print(f\"\\n Round {roun}, cleint {cli}: epoch_accuracy {epoch_accuracy}, epoch_loss {epoch_loss} \\n\")\n",
    "    epoch_train_accuracy=epoch_accuracy\n",
    "    epoch_train_loss=epoch_loss\n",
    "    epoch_test_accuracy, epoch_test_loss= test(f_weights, test_loader)\n",
    "    \n",
    "    \n",
    "    epoch_rmd=model_deviation_function(initial_weights,f_weights)\n",
    "    \n",
    "    #saving data into dataframe\n",
    "    epoch_data = [epoch_train_accuracy, epoch_train_loss, epoch_test_accuracy, epoch_test_loss, epoch_rmd]\n",
    "    epoch_results.loc[len(epoch_results)] = epoch_data\n",
    "    \n",
    "    return epoch_accuracy,epoch_loss, f_weights, epoch_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b882b03e",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5cef1c",
   "metadata": {
    "id": "9d5cef1c"
   },
   "outputs": [],
   "source": [
    "def test(w,data):\n",
    "    lmodel = model().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lmodel.load_state_dict(w)\n",
    "    lmodel.eval()\n",
    "    \n",
    "    #checking the weights\n",
    "    tw = lmodel.state_dict()\n",
    "    #Print(\"Model's before testing the weights in global model\",tw)\n",
    "\n",
    "    # Evaluation phase for test set\n",
    "    acc_list = []\n",
    "    loss_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j, data in enumerate(data, 0):\n",
    "            images, labels = data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            out = lmodel(images)\n",
    "            # Calculate loss\n",
    "            loss = criterion(out, labels)\n",
    "            loss_list.append(loss.item())\n",
    "            #calculate accuracy\n",
    "            acc = accuracy(out, labels)\n",
    "            acc_list.append(acc)\n",
    "    test_loss = np.mean(loss_list)\n",
    "    test_accuracy = np.mean(acc_list)\n",
    "    return test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1aae2",
   "metadata": {},
   "source": [
    "# FL Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d35af3",
   "metadata": {
    "id": "c0d35af3"
   },
   "outputs": [],
   "source": [
    "def federated_learning(i_w, data_client, C, P, R, E, learning_rate, b_size):\n",
    "    \n",
    "    global total_cleints_list, participating_client_list\n",
    "    \n",
    "    global_model.load_state_dict(i_w)\n",
    "    #Print(\"Model's initial weights\", i_w)\n",
    "\n",
    "    #loop for round\n",
    "    for r in range(1,R+1):\n",
    "        round_train_accuracy=0\n",
    "        round_train_loss=0\n",
    "        round_test_accuracy=0\n",
    "        round_test_loss=0\n",
    "        epoch_flag=0\n",
    "\n",
    "        #saving initial weights for spiking model\n",
    "        i_w = {k: v.clone() for k, v in global_model.state_dict().items()}\n",
    "        #Print(\"Model's initial weights\", i_w)\n",
    "        \n",
    "        #colleting weights and results\n",
    "        all_final_weights={}\n",
    "        train_accuracy_list=[]\n",
    "        train_loss_list=[]\n",
    "        \n",
    "        # Randomly select clients\n",
    "        selected_clients = random.sample(total_cleints_list, P)\n",
    "        participating_client_list.append(selected_clients)\n",
    "\n",
    "        #loop for client\n",
    "        for c, data in enumerate(data_client):\n",
    "            \n",
    "            if(c in selected_clients):\n",
    "                \n",
    "                train_loader = torch.utils.data.DataLoader(data, batch_size=b_size, shuffle=True, drop_last=True)\n",
    "                \n",
    "                #train model\n",
    "                train_accuracy, train_loss, c_f_weights, epoch_flag = train(i_w, E, train_loader, learning_rate, c, r,epoch_flag)\n",
    "\n",
    "                train_accuracy_list.append(train_accuracy)\n",
    "                train_loss_list.append(train_loss)\n",
    "\n",
    "                # Accumulate weights for the selected client\n",
    "                for param_name, param_grad in c_f_weights.items():\n",
    "                    if param_name in all_final_weights:\n",
    "                        all_final_weights[param_name] += param_grad\n",
    "                    else:\n",
    "                        all_final_weights[param_name] = param_grad\n",
    "\n",
    "            else:\n",
    "                print(f\"client {c} is not selectecd\")\n",
    "        \n",
    "        round_epoch=(epoch_flag)\n",
    "        \n",
    "        #print(\"Total number of selected clients is\", client_counter)\n",
    "        round_train_loss=sum(train_loss_list)/len(train_loss_list)\n",
    "        round_train_accuracy=sum(train_accuracy_list)/len(train_accuracy_list)\n",
    "\n",
    "        print(f\"Model's Round: {r}, train accuracy of model: {round_train_accuracy}, train loss of model: {round_train_loss} \\n\\n\")\n",
    "\n",
    "        for param_name in all_final_weights:\n",
    "            all_final_weights[param_name] = all_final_weights[param_name].float() / len(selected_clients)\n",
    "\n",
    "        round_test_accuracy, round_test_loss=test(all_final_weights, test_loader)\n",
    "        print(f\"Model's Round: {r}, test accuracy of model: {round_test_accuracy}, test loss of model: {round_test_loss} \\n\\n\")\n",
    "\n",
    "        #model deviation code\n",
    "        round_rmd=model_deviation_function(i_w, all_final_weights)\n",
    "        #print(\"Model deviation values: \", model_deviation)\n",
    "\n",
    "        #saving data into dataframe\n",
    "        round_data = [round_train_accuracy, round_train_loss, round_test_accuracy, round_test_loss, round_rmd, round_epoch]\n",
    "        round_results.loc[len(round_results)] = round_data\n",
    "            \n",
    "        global_model.load_state_dict(all_final_weights)\n",
    "        print(\"round\", r, \"completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512e1d4",
   "metadata": {
    "id": "8512e1d4"
   },
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50345a9f",
   "metadata": {
    "id": "50345a9f"
   },
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fda6c6a",
   "metadata": {
    "id": "7fda6c6a"
   },
   "outputs": [],
   "source": [
    "#===========================Parameters==============================================================\n",
    "client_no=20\n",
    "participating_client=20\n",
    "epochs=5\n",
    "learning_rate=0.01\n",
    "round_no=30\n",
    "batch_size=128\n",
    "distributions = \"non_iid\" # 'non_iid'\n",
    "data_class=10\n",
    "alpha=0.5\n",
    "opti=\"sgd\" # or sgd or adam\n",
    "\n",
    "# List of clients\n",
    "test_loss_epoch=[]\n",
    "\n",
    "total_cleints_list = list(range(0, client_no))\n",
    "# print(total_cleints_list)\n",
    "participating_client_list=[]\n",
    "\n",
    "method=\"fed_adap\"\n",
    "\n",
    "# Define dataframe for round results\n",
    "round_columns = ['train_accuracy', 'train_loss', 'test_accuracy', 'test_loss', 'rmd', 'epoch']\n",
    "round_results = pd.DataFrame(columns=round_columns)\n",
    "\n",
    "# Define dataframe for epoch results\n",
    "epoch_columns = ['train_accuracy', 'train_loss', 'test_accuracy', 'test_loss', 'rmd']\n",
    "epoch_results = pd.DataFrame(columns=epoch_columns)\n",
    "\n",
    "#===================================loading the saved weight list====================================================\n",
    "global_model = model().to(device)\n",
    "file_path = \"s_cnn.pth\"\n",
    "# torch.save(initial_weights, file_path)\n",
    "initial_weights=torch.load(file_path,weights_only=True)\n",
    "Print(\"Model's initial weights\", initial_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a055f33a",
   "metadata": {},
   "source": [
    "<H1>Divide data among cleints</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e49a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load client_datasets from a file\n",
    "if distributions == 'iid':\n",
    "    with open('20_client_datasets_IID.pkl', 'rb') as f:\n",
    "        client_datasets = pickle.load(f)\n",
    "\n",
    "elif distributions == 'non_iid' and alpha==0.5:\n",
    "    with open('20_client_datasets_non_IID_0_5.pkl', 'rb') as f:\n",
    "        client_datasets = pickle.load(f)\n",
    "    \n",
    "elif distributions == 'non_iid' and alpha==0.125:\n",
    "    with open('20_client_datasets_non_IID_0_125.pkl', 'rb') as f:\n",
    "        client_datasets = pickle.load(f)\n",
    "        \n",
    "print(\"client_datasets loaded successfully.\")\n",
    "print_distribution(client_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec69f6d-2802-4aff-8fdc-d8371495125a",
   "metadata": {},
   "source": [
    "<H1>Round zero</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train accuracy for cleints\n",
    "round_train_accuracy=0\n",
    "round_train_loss=0\n",
    "\n",
    "train_accuracy_list=[]\n",
    "train_loss_list=[]\n",
    "for c, data in enumerate(client_datasets):\n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    train_accuracy, train_loss=test(initial_weights, train_loader)\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "    train_loss_list.append(train_loss)\n",
    "round_train_accuracy=(sum(train_accuracy_list)/len(train_accuracy_list))\n",
    "round_train_loss=(sum(train_loss_list)/len(train_loss_list))\n",
    "\n",
    "\n",
    "#test accuracy for server\n",
    "round_test_accuracy=0\n",
    "round_test_loss=0\n",
    "test_accuracy,test_loss=test(initial_weights,test_loader)\n",
    "round_test_accuracy=(test_accuracy)\n",
    "round_test_loss=(test_loss)\n",
    "\n",
    "round_rmd=0\n",
    "round_epoch=0\n",
    "\n",
    "round_data = [round_train_accuracy, round_train_loss, round_test_accuracy, round_test_loss, round_rmd, round_epoch]\n",
    "round_results.loc[len(round_results)] = round_data\n",
    "\n",
    "Print(\"initial_weights\", initial_weights)\n",
    "print(f' train accuracy: {round_train_accuracy}\\n train_loss: {round_train_loss}\\n test_accuracy: {round_test_accuracy}\\n test_loss: {round_test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c3444a",
   "metadata": {},
   "source": [
    "<H1>Run FL</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NJ_CpP0b9OUo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJ_CpP0b9OUo",
    "outputId": "961d3fb1-5fee-4497-be83-fff67f5e10e3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "federated_learning(initial_weights, client_datasets, client_no, participating_client, round_no, epochs, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a91d029",
   "metadata": {},
   "source": [
    "# Reuslts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fa4db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder and file name\n",
    "folder_name = f\"{method}_{opti}_{learning_rate}_{participating_client}_{client_no}_{distributions}_{alpha}\"  # Folder where the Excel file will be saved\n",
    "file_name = \"round_results.xlsx\"\n",
    "\n",
    "\n",
    "# Check if the folder exists, if not, create it\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Full path where the Excel file will be saved\n",
    "file_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "round_results.to_excel(file_path, index=False)\n",
    "\n",
    "print(\"DataFrame successfully written for round results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc1e6ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the folder and file name\n",
    "folder_name =  f\"{method}_{opti}_{learning_rate}_{participating_client}_{client_no}_{distributions}_{alpha}\"   # Folder where the Excel file will be saved\n",
    "file_name = \"epoch_results.xlsx\"\n",
    "\n",
    "# Check if the folder exists, if not, create it\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Full path where the Excel file will be saved\n",
    "file_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "epoch_results.to_excel(file_path, index=False)\n",
    "\n",
    "print(\"DataFrame successfully written for epoch results.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7a26882b",
    "f40fd883",
    "bef90546"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
