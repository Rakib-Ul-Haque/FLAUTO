{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a26882b",
   "metadata": {
    "id": "7a26882b"
   },
   "source": [
    "<H1>Import Libraries</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "473cd34e",
   "metadata": {
    "id": "473cd34e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "from torchvision.datasets import CIFAR10\n",
    "from collections import Counter\n",
    "import random\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from torch import Tensor\n",
    "from typing import Type\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from copy import deepcopy\n",
    "\n",
    "# Set font family for plots\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b21b0f-99de-4780-9213-7a44331d8043",
   "metadata": {},
   "source": [
    "<H1>Load Dataset</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "738ce049",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "738ce049",
    "outputId": "16e7254f-7275-464e-e7cb-552f9fb6ce34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), #This transformation converts a PIL (Python Imaging Library) Image or numpy.ndarray (with shape (H x W x C) in the range [0, 255]) into a PyTorch tensor of shape (C x H x W) in the range [0.0, 1.0]. It essentially rearranges the dimensions of the image data and scales it to a float value between 0 and 1.\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40fd883",
   "metadata": {
    "id": "f40fd883"
   },
   "source": [
    "<H1>IID</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc7a1568",
   "metadata": {
    "id": "fc7a1568"
   },
   "outputs": [],
   "source": [
    "def distribute_dataset_equally(dataset, num_clients):\n",
    "    # Group data by class\n",
    "    class_data = {}\n",
    "    for data, label in dataset:\n",
    "        if label not in class_data:\n",
    "            class_data[label] = []\n",
    "        class_data[label].append((data, label))\n",
    "\n",
    "    # Distribute data\n",
    "    client_data = [[] for _ in range(num_clients)]\n",
    "    for label, data in class_data.items():\n",
    "        data_len = len(data)\n",
    "        base_size = data_len // num_clients\n",
    "        remain = data_len - base_size * num_clients\n",
    "\n",
    "        current_idx = 0\n",
    "        for i in range(num_clients):\n",
    "            end_idx = current_idx + base_size + (1 if i < remain else 0)\n",
    "            client_data[i].extend(data[current_idx:end_idx])\n",
    "            current_idx = end_idx\n",
    "\n",
    "\n",
    "    print_iid_distribution(client_data)\n",
    "    plot_iid_dataset(client_data,num_clients)\n",
    "\n",
    "    return client_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46cb27b0",
   "metadata": {
    "id": "46cb27b0"
   },
   "outputs": [],
   "source": [
    "#print client iid_data distribution\n",
    "def print_iid_distribution(iid_datasets):\n",
    "    # Check if the distribution is correct\n",
    "    for i, client_data in enumerate(iid_datasets):\n",
    "        print(f\"Client {i + 1} data size: {len(client_data)}\")\n",
    "        class_counts = {j: 0 for j in range(10)}\n",
    "        for _, label in client_data:\n",
    "            class_counts[label] += 1\n",
    "        print(f\"Class distribution: {class_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d896660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot client iid_data distribution\n",
    "def plot_iid_dataset(iid_datasets, num_clients):\n",
    "    # CIFAR-10 class names\n",
    "    class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "    # Calculate fraction of distribution for each class across clients\n",
    "    fractions = []\n",
    "    for client_data in iid_datasets:\n",
    "        class_counts = {j: 0 for j in range(10)}\n",
    "        for _, label in client_data:\n",
    "            class_counts[label] += 1\n",
    "        total_data = len(client_data)\n",
    "        fractions.append([class_counts[i] / total_data for i in range(10)])\n",
    "    fractions = np.array(fractions)\n",
    "    # Define colors for each class\n",
    "    colors = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"#FFA500\", \"#FF00FF\", \"#808080\", \"#00FF00\"]\n",
    "    # Generate the 3D bar chart\n",
    "    fig = plt.figure(figsize=(10, 7))  # Adjust the size here\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    xpos, ypos = np.meshgrid(np.arange(num_clients), np.arange(10), indexing=\"ij\")\n",
    "    for i in range(10):\n",
    "        ax.bar3d(xpos[:, i], ypos[:, i], np.zeros_like(xpos[:, i]),0.75, 0.75, fractions[:, i],shade=True, color=colors[i])\n",
    "\n",
    "    ax.set_xlabel('Client Number',labelpad=5)\n",
    "    ax.set_ylabel('CIFAR-10 Classes',labelpad=15)\n",
    "    ax.set_zlabel('Fraction of Distribution',labelpad=5)\n",
    "    ax.set_xticks(np.arange(0.5, num_clients))\n",
    "    ax.set_xticklabels([str(i+1) for i in range(num_clients)], rotation=45)\n",
    "    ax.set_yticks(np.arange(0.5, 10))\n",
    "    ax.set_yticklabels(class_names, rotation=-60)\n",
    "\n",
    "    #ax.view_init(elev=40, azim=60)\n",
    "    ax.view_init(elev=40, azim=10)\n",
    "\n",
    "    plt.subplots_adjust(left=0.01, right=0.99, bottom=0.01, top=0.99)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef90546",
   "metadata": {
    "id": "bef90546"
   },
   "source": [
    "<H1>non-IID</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f7fa1c5",
   "metadata": {
    "id": "1f7fa1c5"
   },
   "outputs": [],
   "source": [
    "#Print the distribution non IID\n",
    "def print_distribution(client_data):\n",
    "    # Check distribution\n",
    "    for l, client_data_value in enumerate(client_data):\n",
    "        print(f\"Client {l + 1} data size: {len(client_data_value)}\")\n",
    "        class_counts = {j: 0 for j in range(10)}\n",
    "        for _, label in client_data_value:\n",
    "            class_counts[label] += 1\n",
    "        print(f\"Class distribution: {class_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8419b107",
   "metadata": {
    "id": "8419b107"
   },
   "outputs": [],
   "source": [
    "# #Non IID code\n",
    "def distribute_dataset_dirichlet(dataset, num_clients, alpha):\n",
    "    # Group data by class\n",
    "    class_data = {}\n",
    "    for data, label in dataset:\n",
    "        if label not in class_data:\n",
    "            class_data[label] = []\n",
    "        class_data[label].append((data, label))\n",
    "\n",
    "    client_data = [[] for _ in range(num_clients)]\n",
    "    for _, data_list in class_data.items():\n",
    "        # Shuffle data for randomness\n",
    "        np.random.shuffle(data_list)\n",
    "\n",
    "        # Get proportions for data split based on Dirichlet distribution\n",
    "        proportions = np.random.dirichlet([alpha]*num_clients)\n",
    "        # print(\"Proportions: \", proportions)\n",
    "        total_data = len(data_list)\n",
    "        # print(\"total_data: \", total_data)\n",
    "        data_splits = [int(proportions[i]*total_data) for i in range(num_clients)]\n",
    "        # print(\"Data_Split: \", data_splits)\n",
    "\n",
    "        # Adjust the splits to account for rounding errors\n",
    "        # print(\"Before data_split:\",data_splits)\n",
    "        data_splits[-1] += total_data - sum(data_splits)\n",
    "        print(\"After data_split:\",data_splits)\n",
    "\n",
    "        start_idx = 0\n",
    "        for i, split in enumerate(data_splits):\n",
    "            end_idx = start_idx + split\n",
    "            client_data[i].extend(data_list[start_idx:end_idx])\n",
    "            start_idx = end_idx\n",
    "    print(\"Client Number: \",num_clients, \"Alpha: \", alpha)\n",
    "    print_distribution(client_data)\n",
    "    plot_distribution(client_data, num_clients )\n",
    "    return client_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62ebf4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the graphs for nonIID\n",
    "def plot_distribution(client_datasets, num_clients):\n",
    "    # CIFAR-10 class names\n",
    "    class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "    # Calculate fraction of distribution for each class across clients\n",
    "    fractions = []\n",
    "    for client_data in client_datasets:\n",
    "        class_counts = {j: 0 for j in range(10)}\n",
    "        for _, label in client_data:\n",
    "            class_counts[label] += 1\n",
    "        total_data = len(client_data)\n",
    "        # print(total_data)\n",
    "        fractions.append([class_counts[i] / total_data for i in range(10)])\n",
    "    fractions = np.array(fractions)\n",
    "    # Define distinct colors for classes using a colormap\n",
    "    colors = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"#FFA500\", \"#FF00FF\", \"#808080\", \"#00FF00\"]\n",
    "    # # Generate the 3D bar chart\n",
    "    # fig = plt.figure(figsize=(18, 12))\n",
    "    # ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Generate the 3D bar chart\n",
    "    fig = plt.figure(figsize=(10, 7))  # Adjust the size here\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    xpos, ypos = np.meshgrid(np.arange(num_clients), np.arange(10), indexing=\"ij\")\n",
    "\n",
    "    for i in range(10):\n",
    "        ax.bar3d(xpos[:, i], ypos[:, i], np.zeros_like(xpos[:, i]),0.75, 0.75, fractions[:, i],shade=True, color=colors[i])\n",
    "\n",
    "    ax.set_xlabel('Client Number')\n",
    "    ax.set_ylabel('CIFAR-10 Classes')\n",
    "    ax.set_zlabel('Fraction of Distribution')\n",
    "    ax.set_title('Distribution of CIFAR-10 Classes across Clients based on Dirichlet Distribution')\n",
    "    ax.set_xticks(np.arange(0.5, num_clients))\n",
    "    ax.set_xticklabels([str(i+1) for i in range(num_clients)], fontsize=12)\n",
    "    ax.set_yticks(np.arange(0.5, 10))\n",
    "    ax.set_yticklabels(class_names)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f95a6",
   "metadata": {},
   "source": [
    "# Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2700fb3",
   "metadata": {
    "id": "f2700fb3"
   },
   "outputs": [],
   "source": [
    "def accuracy(outp, target):\n",
    "    \"\"\"Computes accuracy\"\"\"\n",
    "    with torch.no_grad():\n",
    "        pred = torch.argmax(outp, dim=1)\n",
    "        correct = pred.eq(target).float().sum().item()\n",
    "        return 100.0 * correct / target.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9306ada",
   "metadata": {
    "id": "b9306ada"
   },
   "outputs": [],
   "source": [
    "def Print(string, dictionary):\n",
    "    first_key = next(iter(dictionary))\n",
    "    first_value = dictionary[first_key]\n",
    "    print(f\"{string}:{first_key}: {first_value[0][0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0db37362",
   "metadata": {
    "id": "0db37362"
   },
   "outputs": [],
   "source": [
    "def forbinus_norm_function(w_i):\n",
    "    value = 0\n",
    "    for k in w_i.keys():\n",
    "        value += torch.linalg.norm(w_i[k])\n",
    "    return value.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e387f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_deviation_function(w_i, w_f):\n",
    "    model_deviation = 0\n",
    "    for k in w_i.keys():\n",
    "        model_deviation += torch.linalg.norm(w_f[k].to(torch.float) - w_i[k].to(torch.float)) / torch.linalg.norm(w_i[k].to(torch.float))\n",
    "    #print(model_deviation.item())\n",
    "    return model_deviation.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53678293",
   "metadata": {
    "id": "53678293"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0a3ffbe",
   "metadata": {
    "id": "e0a3ffbe"
   },
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 10)  # Change output to 10 for CIFAR-10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.relu4(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu5(self.fc1(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7eaf8a",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "833d43d8",
   "metadata": {
    "id": "833d43d8"
   },
   "outputs": [],
   "source": [
    "def train(i_weights, epochs, train_loader, le_rate, cli,roun, epoch_flag):\n",
    "    global opti\n",
    "    \n",
    "    local_model = model().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if opti==\"adam\":\n",
    "        optimizer = torch.optim.Adam(local_model.parameters(), lr=le_rate,  weight_decay=0.004)\n",
    "    elif opti==\"sgd\":\n",
    "        optimizer = torch.optim.SGD(local_model.parameters(), lr=le_rate, weight_decay=0.004)\n",
    "    \n",
    "    epoch_train_accuracy=0 \n",
    "    epoch_train_loss=0\n",
    "    epoch_test_accuracy=0\n",
    "    epoch_test_loss=0\n",
    "    epoch_rmd=0\n",
    "\n",
    "    local_model.load_state_dict(i_weights)\n",
    "\n",
    "    local_model.train()  # Set the model to training mode\n",
    "\n",
    "    # initial weights cathing and printing\n",
    "    initial_weights = {k: v.clone() for k, v in local_model.state_dict().items()}\n",
    "    #Print(\"Model's inside the function Initial weights for client\",initial_weights)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_flag=epoch_flag+1\n",
    "        # gradients_this_epoch = {}\n",
    "        total_samples = 0\n",
    "        total_loss=0\n",
    "        correct_samples = 0\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward + backward + optimize\n",
    "            outputs = local_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = outputs.max(1)  # Get the index of the maximum value in outputs (predicted class)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_samples += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        if(total_samples!=0 and len(train_loader)!=0):\n",
    "            epoch_accuracy = 100 * correct_samples / total_samples\n",
    "            epoch_loss = total_loss / len(train_loader)\n",
    "        else:\n",
    "            epoch_accuracy = 100 * correct_samples / (total_samples+1)\n",
    "            epoch_loss = total_loss / (len(train_loader)+1)\n",
    "        print(f\"Round {roun}, cleint {cli+1}, epoch {epoch+1}: epoch_accuracy {epoch_accuracy}, epoch_loss {epoch_loss} \")\n",
    "    \n",
    "    f_weights = {k: v.clone() for k, v in local_model.state_dict().items()}\n",
    "\n",
    "    #print(f\"\\n Round {roun}, cleint {cli}: epoch_accuracy {epoch_accuracy}, epoch_loss {epoch_loss} \\n\")\n",
    "    epoch_train_accuracy=epoch_accuracy\n",
    "    epoch_train_loss=epoch_loss\n",
    "    epoch_test_accuracy, epoch_test_loss= test(f_weights, test_loader)\n",
    "    \n",
    "    \n",
    "    epoch_rmd=model_deviation_function(initial_weights,f_weights)\n",
    "    \n",
    "    #saving data into dataframe\n",
    "    epoch_data = [epoch_train_accuracy, epoch_train_loss, epoch_test_accuracy, epoch_test_loss, epoch_rmd]\n",
    "    epoch_results.loc[len(epoch_results)] = epoch_data\n",
    "    \n",
    "    model_update = {}\n",
    "    for key in local_model.state_dict():\n",
    "        model_update[key] = torch.sub(i_weights[key], f_weights[key])\n",
    "    \n",
    "    return epoch_accuracy,epoch_loss, epoch_flag, model_update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b882b03e",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d5cef1c",
   "metadata": {
    "id": "9d5cef1c"
   },
   "outputs": [],
   "source": [
    "def test(w,data):\n",
    "    lmodel = model().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()  # Assuming a classification task\n",
    "    #optimizer = torch.optim.SGD(lmodel.parameters(), lr=learning_rate)\n",
    "    lmodel.load_state_dict(w)\n",
    "    lmodel.eval()\n",
    "\n",
    "    #checking the weights\n",
    "    tw = lmodel.state_dict()\n",
    "    #Print(\"Model's before testing the weights in global model\",tw)\n",
    "\n",
    "    # Evaluation phase for test set\n",
    "    acc_list = []\n",
    "    loss_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j, data in enumerate(data, 0):\n",
    "            images, labels = data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            out = lmodel(images)\n",
    "            # Calculate loss\n",
    "            loss = criterion(out, labels)\n",
    "            loss_list.append(loss.item())\n",
    "            #calculate accuracy\n",
    "            acc = accuracy(out, labels)\n",
    "            acc_list.append(acc)\n",
    "    test_loss = np.mean(loss_list)\n",
    "    test_accuracy = np.mean(acc_list)\n",
    "    #print(\"Model's Test accuracy : {:.2f}%\".format(test_accuracy))\n",
    "    return test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1aae2",
   "metadata": {},
   "source": [
    "# FL Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d44b455c-2a8c-4b29-8b45-6755d8e9ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_updates(w, n_k):\n",
    "    w_avg = deepcopy(w[0])\n",
    "    for key in w_avg.keys():\n",
    "        w_avg[key] = torch.mul(w_avg[key], n_k[0])\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] = torch.add(w_avg[key], w[i][key], alpha=n_k[i])\n",
    "        w_avg[key] = torch.div(w_avg[key], sum(n_k))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f726a08-2e1c-43d9-b711-c6a200ff1a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_learning(i_w, data_client, C, P, R, E, learning_rate, b_size, beta_1=0.9, beta_2=0.999, epsilon=1e-8):\n",
    "    \n",
    "    global total_clients_list, participating_client_list, m, v, t\n",
    "    \n",
    "    global_model.load_state_dict(i_w)\n",
    "    t = 0  # Time step for FedAdam\n",
    "\n",
    "    # loop for round\n",
    "    for r in range(1, R+1):\n",
    "        round_train_accuracy = 0\n",
    "        round_train_loss = 0\n",
    "        round_test_accuracy = 0\n",
    "        round_test_loss = 0\n",
    "        epoch_flag = 0\n",
    "\n",
    "        # Saving initial weights for spiking model\n",
    "        i_w = {k: v.clone() for k, v in global_model.state_dict().items()}\n",
    "\n",
    "        # Collecting weights and results\n",
    "        data_size = []\n",
    "        all_clients_updates = []\n",
    "        train_accuracy_list = []\n",
    "        train_loss_list = []\n",
    "        \n",
    "        # Randomly select clients\n",
    "        selected_clients = random.sample(total_clients_list, P)\n",
    "        participating_client_list.append(selected_clients)\n",
    "\n",
    "        # Loop for client\n",
    "        for c, data in enumerate(data_client):\n",
    "            if c in selected_clients:\n",
    "                train_loader = torch.utils.data.DataLoader(data, batch_size=b_size, shuffle=True, drop_last=True)\n",
    "                \n",
    "                # Train model\n",
    "                train_accuracy, train_loss, epoch_flag, model_update = train(i_w, E, train_loader, learning_rate, c, r, epoch_flag)\n",
    "\n",
    "                train_accuracy_list.append(train_accuracy)\n",
    "                train_loss_list.append(train_loss)\n",
    "                \n",
    "                all_clients_updates.append(model_update)\n",
    "                data_size.append(len(train_loader))\n",
    "            else:\n",
    "                print(f\"Client {c+1} is not selected\")\n",
    "        \n",
    "        round_epoch=epoch_flag\n",
    "        round_train_loss=sum(train_loss_list)/len(train_loss_list)\n",
    "        round_train_accuracy=sum(train_accuracy_list)/len(train_accuracy_list)\n",
    "        print(f\"Model's Round: {r}, train accuracy of model: {round_train_accuracy}, train loss of model: {round_train_loss} \\n\\n\")\n",
    "\n",
    "        # Aggregate the updates using a weighted average\n",
    "        update_avg = average_updates(all_clients_updates, data_size)\n",
    "\n",
    "        # Initialize moment vectors if not done\n",
    "        if m is None:\n",
    "            m = {key: torch.zeros_like(param) for key, param in update_avg.items()}\n",
    "            v = {key: torch.zeros_like(param) for key, param in update_avg.items()}\n",
    "\n",
    "        # Adam update rule for each parameter\n",
    "        t += 1\n",
    "        for key in i_w:\n",
    "            # Update biased first moment estimate\n",
    "            m[key] = beta_1 * m[key] + (1 - beta_1) * update_avg[key]\n",
    "            # Update biased second raw moment estimate\n",
    "            v[key] = beta_2 * v[key] + (1 - beta_2) * (update_avg[key] ** 2)\n",
    "            \n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_hat = m[key] / (1 - beta_1 ** t)\n",
    "            # Compute bias-corrected second raw moment estimate\n",
    "            v_hat = v[key] / (1 - beta_2 ** t)\n",
    "            \n",
    "            # Update weights using Adam rule\n",
    "            i_w[key] -= learning_rate * m_hat / (torch.sqrt(v_hat) + epsilon)\n",
    "\n",
    "        # Test the model on global test set\n",
    "        round_test_accuracy, round_test_loss = test(i_w, test_loader)\n",
    "        print(f\"Model's Round: {r}, test accuracy of model: {round_test_accuracy}, test loss of model: {round_test_loss} \\n\\n\")\n",
    "\n",
    "        # Model deviation calculation\n",
    "        round_rmd = model_deviation_function(i_w, global_model.state_dict())\n",
    "        \n",
    "        # Save data into dataframe\n",
    "        round_data = [round_train_accuracy, round_train_loss, round_test_accuracy, round_test_loss, round_rmd, epoch_flag]\n",
    "        round_results.loc[len(round_results)] = round_data\n",
    "\n",
    "        # Load the updated weights into the global model\n",
    "        global_model.load_state_dict(i_w)\n",
    "        print(\"Round\", r, \"completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0d35af3",
   "metadata": {
    "id": "c0d35af3"
   },
   "outputs": [],
   "source": [
    "# def federated_learning(i_w, data_client, C, P, R, E, learning_rate, b_size):\n",
    "    \n",
    "#     global total_cleints_list, participating_client_list, v\n",
    "    \n",
    "#     global_model.load_state_dict(i_w)\n",
    "#     #Print(\"Model's initial weights\", i_w)\n",
    "\n",
    "#     #loop for round\n",
    "#     for r in range(1,R+1):\n",
    "#         round_train_accuracy=0\n",
    "#         round_train_loss=0\n",
    "#         round_test_accuracy=0\n",
    "#         round_test_loss=0\n",
    "#         epoch_flag=0\n",
    "\n",
    "#         #saving initial weights for spiking model\n",
    "#         i_w = {k: v.clone() for k, v in global_model.state_dict().items()}\n",
    "#         #Print(\"Model's initial weights\", i_w)\n",
    "        \n",
    "#         #colleting weights and results\n",
    "#         data_size=[]\n",
    "#         all_final_weights={}\n",
    "#         all_cleints_updates=[]\n",
    "#         train_accuracy_list=[]\n",
    "#         train_loss_list=[]\n",
    "        \n",
    "#         # Randomly select clients\n",
    "#         selected_clients = random.sample(total_cleints_list, P)\n",
    "#         participating_client_list.append(selected_clients)\n",
    "\n",
    "#         #loop for client\n",
    "#         for c, data in enumerate(data_client):\n",
    "            \n",
    "#             if(c in selected_clients):\n",
    "                \n",
    "#                 train_loader = torch.utils.data.DataLoader(data, batch_size=b_size, shuffle=True, drop_last=True)\n",
    "                \n",
    "#                 #train model\n",
    "#                 train_accuracy, train_loss, epoch_flag, model_update = train(i_w, E, train_loader, learning_rate, c, r,epoch_flag)\n",
    "\n",
    "#                 train_accuracy_list.append(train_accuracy)\n",
    "#                 train_loss_list.append(train_loss)\n",
    "                \n",
    "#                 all_cleints_updates.append(model_update)\n",
    "\n",
    "#                 data_size.append(len(train_loader))\n",
    "                \n",
    "                \n",
    "#                 # Accumulate weights for the selected client\n",
    "#                 # for param_name, param_grad in c_f_weights.items():\n",
    "#                 #     if param_name in all_final_weights:\n",
    "#                 #         all_final_weights[param_name] += param_grad\n",
    "#                 #     else:\n",
    "#                 #         all_final_weights[param_name] = param_grad\n",
    "\n",
    "#             else:\n",
    "#                 print(f\"client {c} is not selectecd\")\n",
    "        \n",
    "#         round_epoch=(epoch_flag)\n",
    "#         #print(\"Total number of selected clients is\", client_counter)\n",
    "#         round_train_loss=sum(train_loss_list)/len(train_loss_list)\n",
    "#         round_train_accuracy=sum(train_accuracy_list)/len(train_accuracy_list)\n",
    "#         print(f\"Model's Round: {r}, train accuracy of model: {round_train_accuracy}, train loss of model: {round_train_loss} \\n\\n\")\n",
    "\n",
    "#         update_avg = average_updates( all_cleints_updates, data_size)\n",
    "\n",
    "#         if v is None:\n",
    "#             v = deepcopy(update_avg)\n",
    "#         else:\n",
    "#             for key in v.keys():\n",
    "#                 v[key] = update_avg[key] + (v[key] * 0.99)\n",
    "                \n",
    "#         for key in i_w:\n",
    "#             all_final_weights[key]=i_w[key] - (v[key] * 1)\n",
    "\n",
    "#         # for param_name in all_final_weights:\n",
    "#         #     all_final_weights[param_name] = all_final_weights[param_name].float() / len(selected_clients)\n",
    "\n",
    "#         round_test_accuracy, round_test_loss=test(all_final_weights, test_loader)\n",
    "#         print(f\"Model's Round: {r}, test accuracy of model: {round_test_accuracy}, test loss of model: {round_test_loss} \\n\\n\")\n",
    "\n",
    "#         #model deviation code\n",
    "#         round_rmd=model_deviation_function(i_w, all_final_weights)\n",
    "#         #print(\"Model deviation values: \", model_deviation)\n",
    "\n",
    "#         #saving data into dataframe\n",
    "#         round_data = [round_train_accuracy, round_train_loss, round_test_accuracy, round_test_loss, round_rmd, round_epoch]\n",
    "#         round_results.loc[len(round_results)] = round_data\n",
    "            \n",
    "#         global_model.load_state_dict(all_final_weights)\n",
    "#         print(\"round\", r, \"completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512e1d4",
   "metadata": {
    "id": "8512e1d4"
   },
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50345a9f",
   "metadata": {
    "id": "50345a9f"
   },
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fda6c6a",
   "metadata": {
    "id": "7fda6c6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's initial weights:conv1.weight: tensor([[-0.0619,  0.0628, -0.1613],\n",
      "        [-0.1879,  0.0229,  0.1701],\n",
      "        [-0.1736, -0.0053,  0.0203]], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#===========================Parameters==============================================================\n",
    "client_no=20\n",
    "participating_client=20\n",
    "epochs=5\n",
    "learning_rate=0.01\n",
    "round_no=30\n",
    "batch_size=128\n",
    "distributions = \"non_iid\" # 'non_iid'\n",
    "data_class=10\n",
    "alpha=0.5#\"infinity\"\n",
    "opti=\"sgd\" # or sgd or adam\n",
    "v=None\n",
    "m=None\n",
    "t=0\n",
    "method=\"fed_adam\"\n",
    "\n",
    "# List of clients\n",
    "total_clients_list = list(range(0, client_no))\n",
    "# print(total_cleints_list)\n",
    "participating_client_list=[]\n",
    "\n",
    "# Define dataframe for round results\n",
    "round_columns = ['train_accuracy', 'train_loss', 'test_accuracy', 'test_loss', 'rmd', 'epoch']\n",
    "round_results = pd.DataFrame(columns=round_columns)\n",
    "\n",
    "# Define dataframe for epoch results\n",
    "epoch_columns = ['train_accuracy', 'train_loss', 'test_accuracy', 'test_loss', 'rmd']\n",
    "epoch_results = pd.DataFrame(columns=epoch_columns)\n",
    "\n",
    "#===================================loading the saved weight list====================================================\n",
    "global_model = model().to(device)\n",
    "# initial_weights={k: v.clone() for k, v in global_model.state_dict().items()}\n",
    "# Save the initial weights\n",
    "file_path = \"s_cnn.pth\"\n",
    "# torch.save(initial_weights, file_path)\n",
    "initial_weights=torch.load(file_path,weights_only=True)\n",
    "Print(\"Model's initial weights\", initial_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a055f33a",
   "metadata": {},
   "source": [
    "<H1>Divide data among cleints</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3f8ad92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #=================================loading IID data===========================\n",
    "# if distributions == 'iid':\n",
    "#     client_datasets = distribute_dataset_equally(train_dataset,client_no)\n",
    "    \n",
    "# elif distributions == 'non_iid':\n",
    "#     client_datasets = distribute_dataset_dirichlet(train_dataset, client_no, alpha)\n",
    "# else:\n",
    "#     print(\"provide a valid distribution Please\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e51410b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save client_datasets to a file\n",
    "# if distributions == 'iid':\n",
    "#     with open('client_datasets_IID.pkl', 'wb') as f:\n",
    "#         pickle.dump(client_datasets, f)\n",
    "# elif distributions == 'non_iid' and alpha==0.5:\n",
    "#     with open('client_datasets_non_IID_0_5.pkl', 'wb') as f:\n",
    "#         pickle.dump(client_datasets, f)\n",
    "# elif distributions == 'non_iid' and alpha==0.125:\n",
    "#     with open('client_datasets_non_IID_0_125.pkl', 'wb') as f:\n",
    "#         pickle.dump(client_datasets, f)\n",
    "\n",
    "# print(\"client_datasets saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e49a42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_datasets loaded successfully.\n",
      "Client 1 data size: 1928\n",
      "Class distribution: {0: 93, 1: 405, 2: 144, 3: 3, 4: 342, 5: 365, 6: 25, 7: 64, 8: 248, 9: 239}\n",
      "Client 2 data size: 1997\n",
      "Class distribution: {0: 38, 1: 0, 2: 921, 3: 588, 4: 14, 5: 9, 6: 58, 7: 51, 8: 11, 9: 307}\n",
      "Client 3 data size: 3557\n",
      "Class distribution: {0: 849, 1: 41, 2: 2, 3: 208, 4: 645, 5: 640, 6: 853, 7: 151, 8: 20, 9: 148}\n",
      "Client 4 data size: 1954\n",
      "Class distribution: {0: 369, 1: 91, 2: 9, 3: 135, 4: 108, 5: 65, 6: 267, 7: 30, 8: 673, 9: 207}\n",
      "Client 5 data size: 3332\n",
      "Class distribution: {0: 25, 1: 0, 2: 866, 3: 1163, 4: 6, 5: 194, 6: 13, 7: 375, 8: 61, 9: 629}\n",
      "Client 6 data size: 2673\n",
      "Class distribution: {0: 241, 1: 643, 2: 161, 3: 26, 4: 2, 5: 861, 6: 65, 7: 182, 8: 462, 9: 30}\n",
      "Client 7 data size: 2750\n",
      "Class distribution: {0: 161, 1: 0, 2: 503, 3: 26, 4: 184, 5: 443, 6: 6, 7: 6, 8: 1412, 9: 9}\n",
      "Client 8 data size: 2071\n",
      "Class distribution: {0: 217, 1: 128, 2: 136, 3: 37, 4: 20, 5: 19, 6: 42, 7: 371, 8: 37, 9: 1064}\n",
      "Client 9 data size: 2185\n",
      "Class distribution: {0: 0, 1: 286, 2: 426, 3: 441, 4: 223, 5: 651, 6: 5, 7: 23, 8: 107, 9: 23}\n",
      "Client 10 data size: 1544\n",
      "Class distribution: {0: 602, 1: 10, 2: 2, 3: 16, 4: 91, 5: 47, 6: 299, 7: 192, 8: 255, 9: 30}\n",
      "Client 11 data size: 681\n",
      "Class distribution: {0: 2, 1: 43, 2: 1, 3: 75, 4: 26, 5: 75, 6: 101, 7: 1, 8: 205, 9: 152}\n",
      "Client 12 data size: 1524\n",
      "Class distribution: {0: 10, 1: 2, 2: 79, 3: 142, 4: 1, 5: 6, 6: 670, 7: 90, 8: 1, 9: 523}\n",
      "Client 13 data size: 1346\n",
      "Class distribution: {0: 281, 1: 82, 2: 8, 3: 208, 4: 116, 5: 206, 6: 5, 7: 128, 8: 158, 9: 154}\n",
      "Client 14 data size: 4708\n",
      "Class distribution: {0: 596, 1: 249, 2: 1220, 3: 246, 4: 932, 5: 595, 6: 18, 7: 717, 8: 33, 9: 102}\n",
      "Client 15 data size: 1893\n",
      "Class distribution: {0: 30, 1: 9, 2: 65, 3: 842, 4: 145, 5: 0, 6: 605, 7: 34, 8: 64, 9: 99}\n",
      "Client 16 data size: 2479\n",
      "Class distribution: {0: 0, 1: 924, 2: 71, 3: 176, 4: 10, 5: 259, 6: 54, 7: 897, 8: 0, 9: 88}\n",
      "Client 17 data size: 2031\n",
      "Class distribution: {0: 78, 1: 165, 2: 14, 3: 412, 4: 10, 5: 397, 6: 69, 7: 1, 8: 643, 9: 242}\n",
      "Client 18 data size: 6129\n",
      "Class distribution: {0: 673, 1: 1721, 2: 41, 3: 127, 4: 1129, 5: 118, 6: 29, 7: 1383, 8: 100, 9: 808}\n",
      "Client 19 data size: 1348\n",
      "Class distribution: {0: 70, 1: 153, 2: 1, 3: 96, 4: 46, 5: 35, 6: 541, 7: 159, 8: 139, 9: 108}\n",
      "Client 20 data size: 3870\n",
      "Class distribution: {0: 665, 1: 48, 2: 330, 3: 33, 4: 950, 5: 15, 6: 1275, 7: 145, 8: 371, 9: 38}\n"
     ]
    }
   ],
   "source": [
    "# Load client_datasets from a file\n",
    "if distributions == 'iid':\n",
    "    with open('20_client_datasets_IID.pkl', 'rb') as f:\n",
    "        client_datasets = pickle.load(f)\n",
    "\n",
    "elif distributions == 'non_iid' and alpha==0.5:\n",
    "    with open('20_client_datasets_non_IID_0_5.pkl', 'rb') as f:\n",
    "        client_datasets = pickle.load(f)\n",
    "    \n",
    "elif distributions == 'non_iid' and alpha==0.125:\n",
    "    with open('20_client_datasets_non_IID_0_125.pkl', 'rb') as f:\n",
    "        client_datasets = pickle.load(f)\n",
    "        \n",
    "print(\"client_datasets loaded successfully.\")\n",
    "print_distribution(client_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec69f6d-2802-4aff-8fdc-d8371495125a",
   "metadata": {},
   "source": [
    "<H1>Round zero</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "516b645b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_weights:conv1.weight: tensor([[-0.0619,  0.0628, -0.1613],\n",
      "        [-0.1879,  0.0229,  0.1701],\n",
      "        [-0.1736, -0.0053,  0.0203]], device='cuda:0')\n",
      "\n",
      " train accuracy: 11.248010908070665\n",
      " train_loss: 2.3002412608415588\n",
      " test_accuracy: 10.013977635782748\n",
      " test_loss: 2.3027472800720994\n"
     ]
    }
   ],
   "source": [
    "#train accuracy for cleints\n",
    "round_train_accuracy=0\n",
    "round_train_loss=0\n",
    "\n",
    "train_accuracy_list=[]\n",
    "train_loss_list=[]\n",
    "for c, data in enumerate(client_datasets):\n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    train_accuracy, train_loss=test(initial_weights, train_loader)\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "    train_loss_list.append(train_loss)\n",
    "round_train_accuracy=(sum(train_accuracy_list)/len(train_accuracy_list))\n",
    "round_train_loss=(sum(train_loss_list)/len(train_loss_list))\n",
    "\n",
    "\n",
    "#test accuracy for server\n",
    "round_test_accuracy=0\n",
    "round_test_loss=0\n",
    "test_accuracy,test_loss=test(initial_weights,test_loader)\n",
    "round_test_accuracy=(test_accuracy)\n",
    "round_test_loss=(test_loss)\n",
    "\n",
    "round_rmd=0\n",
    "round_epoch=0\n",
    "\n",
    "round_data = [round_train_accuracy, round_train_loss, round_test_accuracy, round_test_loss, round_rmd, round_epoch]\n",
    "round_results.loc[len(round_results)] = round_data\n",
    "\n",
    "Print(\"initial_weights\", initial_weights)\n",
    "print(f' train accuracy: {round_train_accuracy}\\n train_loss: {round_train_loss}\\n test_accuracy: {round_test_accuracy}\\n test_loss: {round_test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c3444a",
   "metadata": {},
   "source": [
    "<H1>Run FL</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NJ_CpP0b9OUo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJ_CpP0b9OUo",
    "outputId": "961d3fb1-5fee-4497-be83-fff67f5e10e3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1, cleint 1, epoch 1: epoch_accuracy 7.03125, epoch_loss 2.297184610366821 \n",
      "Round 1, cleint 1, epoch 2: epoch_accuracy 18.90625, epoch_loss 2.2742557366689047 \n",
      "Round 1, cleint 1, epoch 3: epoch_accuracy 20.520833333333332, epoch_loss 2.2491801579793296 \n",
      "Round 1, cleint 1, epoch 4: epoch_accuracy 20.989583333333332, epoch_loss 2.214905309677124 \n",
      "Round 1, cleint 1, epoch 5: epoch_accuracy 20.885416666666668, epoch_loss 2.165635331471761 \n",
      "Round 1, cleint 2, epoch 1: epoch_accuracy 38.4375, epoch_loss 2.2232197443644206 \n",
      "Round 1, cleint 2, epoch 2: epoch_accuracy 45.572916666666664, epoch_loss 2.088173818588257 \n",
      "Round 1, cleint 2, epoch 3: epoch_accuracy 45.885416666666664, epoch_loss 1.7613770961761475 \n",
      "Round 1, cleint 2, epoch 4: epoch_accuracy 46.09375, epoch_loss 1.4712892691294353 \n",
      "Round 1, cleint 2, epoch 5: epoch_accuracy 44.739583333333336, epoch_loss 1.41397971312205 \n",
      "Round 1, cleint 3, epoch 1: epoch_accuracy 14.496527777777779, epoch_loss 2.2897431673826993 \n",
      "Round 1, cleint 3, epoch 2: epoch_accuracy 25.17361111111111, epoch_loss 2.201409622474953 \n",
      "Round 1, cleint 3, epoch 3: epoch_accuracy 25.752314814814813, epoch_loss 2.0078282532868563 \n",
      "Round 1, cleint 3, epoch 4: epoch_accuracy 24.479166666666668, epoch_loss 1.8785948311841045 \n",
      "Round 1, cleint 3, epoch 5: epoch_accuracy 26.099537037037038, epoch_loss 1.8521873774351898 \n",
      "Round 1, cleint 4, epoch 1: epoch_accuracy 22.1875, epoch_loss 2.283281707763672 \n",
      "Round 1, cleint 4, epoch 2: epoch_accuracy 34.010416666666664, epoch_loss 2.2439616680145265 \n",
      "Round 1, cleint 4, epoch 3: epoch_accuracy 34.322916666666664, epoch_loss 2.1925416151682535 \n",
      "Round 1, cleint 4, epoch 4: epoch_accuracy 34.479166666666664, epoch_loss 2.117094151178996 \n",
      "Round 1, cleint 4, epoch 5: epoch_accuracy 34.635416666666664, epoch_loss 2.0127556641896565 \n",
      "Round 1, cleint 5, epoch 1: epoch_accuracy 34.375, epoch_loss 2.22261696595412 \n",
      "Round 1, cleint 5, epoch 2: epoch_accuracy 34.85576923076923, epoch_loss 2.0252013298181386 \n",
      "Round 1, cleint 5, epoch 3: epoch_accuracy 34.61538461538461, epoch_loss 1.6962434695317194 \n",
      "Round 1, cleint 5, epoch 4: epoch_accuracy 33.83413461538461, epoch_loss 1.6259215611677904 \n",
      "Round 1, cleint 5, epoch 5: epoch_accuracy 34.64543269230769, epoch_loss 1.6038262431438153 \n",
      "Round 1, cleint 6, epoch 1: epoch_accuracy 17.109375, epoch_loss 2.280193054676056 \n",
      "Round 1, cleint 6, epoch 2: epoch_accuracy 31.484375, epoch_loss 2.2026829361915587 \n",
      "Round 1, cleint 6, epoch 3: epoch_accuracy 32.421875, epoch_loss 2.0455137848854066 \n",
      "Round 1, cleint 6, epoch 4: epoch_accuracy 32.0703125, epoch_loss 1.8585331916809082 \n",
      "Round 1, cleint 6, epoch 5: epoch_accuracy 32.421875, epoch_loss 1.8043066143989563 \n",
      "Round 1, cleint 7, epoch 1: epoch_accuracy 41.44345238095238, epoch_loss 2.23457586197626 \n",
      "Round 1, cleint 7, epoch 2: epoch_accuracy 51.22767857142857, epoch_loss 2.0298306544621787 \n",
      "Round 1, cleint 7, epoch 3: epoch_accuracy 51.26488095238095, epoch_loss 1.6052880116871424 \n",
      "Round 1, cleint 7, epoch 4: epoch_accuracy 51.19047619047619, epoch_loss 1.4674489384605771 \n",
      "Round 1, cleint 7, epoch 5: epoch_accuracy 51.711309523809526, epoch_loss 1.4133093640917824 \n",
      "Round 1, cleint 8, epoch 1: epoch_accuracy 41.162109375, epoch_loss 2.2490160316228867 \n",
      "Round 1, cleint 8, epoch 2: epoch_accuracy 51.513671875, epoch_loss 2.097091890871525 \n",
      "Round 1, cleint 8, epoch 3: epoch_accuracy 51.416015625, epoch_loss 1.7621334865689278 \n",
      "Round 1, cleint 8, epoch 4: epoch_accuracy 51.26953125, epoch_loss 1.5938782095909119 \n",
      "Round 1, cleint 8, epoch 5: epoch_accuracy 51.3671875, epoch_loss 1.5595810562372208 \n",
      "Round 1, cleint 9, epoch 1: epoch_accuracy 19.990808823529413, epoch_loss 2.2693174446330353 \n",
      "Round 1, cleint 9, epoch 2: epoch_accuracy 23.529411764705884, epoch_loss 2.2073554712183334 \n",
      "Round 1, cleint 9, epoch 3: epoch_accuracy 29.136029411764707, epoch_loss 2.1010950453141155 \n",
      "Round 1, cleint 9, epoch 4: epoch_accuracy 29.917279411764707, epoch_loss 1.924663733033573 \n",
      "Round 1, cleint 9, epoch 5: epoch_accuracy 30.009191176470587, epoch_loss 1.8290210611679976 \n",
      "Round 1, cleint 10, epoch 1: epoch_accuracy 8.984375, epoch_loss 2.3004130721092224 \n",
      "Round 1, cleint 10, epoch 2: epoch_accuracy 37.760416666666664, epoch_loss 2.2480013569196067 \n",
      "Round 1, cleint 10, epoch 3: epoch_accuracy 38.8671875, epoch_loss 2.1766791145006814 \n",
      "Round 1, cleint 10, epoch 4: epoch_accuracy 38.932291666666664, epoch_loss 2.063204050064087 \n",
      "Round 1, cleint 10, epoch 5: epoch_accuracy 39.0625, epoch_loss 1.883615146080653 \n",
      "Round 1, cleint 11, epoch 1: epoch_accuracy 12.5, epoch_loss 2.285065507888794 \n",
      "Round 1, cleint 11, epoch 2: epoch_accuracy 19.53125, epoch_loss 2.2737027645111083 \n",
      "Round 1, cleint 11, epoch 3: epoch_accuracy 25.625, epoch_loss 2.2605352878570555 \n",
      "Round 1, cleint 11, epoch 4: epoch_accuracy 30.625, epoch_loss 2.247445011138916 \n",
      "Round 1, cleint 11, epoch 5: epoch_accuracy 29.21875, epoch_loss 2.2360256195068358 \n",
      "Round 1, cleint 12, epoch 1: epoch_accuracy 22.585227272727273, epoch_loss 2.2648292888294566 \n",
      "Round 1, cleint 12, epoch 2: epoch_accuracy 41.54829545454545, epoch_loss 2.1851251775568183 \n",
      "Round 1, cleint 12, epoch 3: epoch_accuracy 44.67329545454545, epoch_loss 2.0443696433847602 \n",
      "Round 1, cleint 12, epoch 4: epoch_accuracy 44.17613636363637, epoch_loss 1.7218195741826838 \n",
      "Round 1, cleint 12, epoch 5: epoch_accuracy 44.03409090909091, epoch_loss 1.4450276223095981 \n",
      "Round 1, cleint 13, epoch 1: epoch_accuracy 14.609375, epoch_loss 2.3008718729019164 \n",
      "Round 1, cleint 13, epoch 2: epoch_accuracy 15.15625, epoch_loss 2.28979651927948 \n",
      "Round 1, cleint 13, epoch 3: epoch_accuracy 15.234375, epoch_loss 2.280098557472229 \n",
      "Round 1, cleint 13, epoch 4: epoch_accuracy 16.953125, epoch_loss 2.268156886100769 \n",
      "Round 1, cleint 13, epoch 5: epoch_accuracy 15.859375, epoch_loss 2.257189989089966 \n",
      "Round 1, cleint 14, epoch 1: epoch_accuracy 22.4609375, epoch_loss 2.2750274538993835 \n",
      "Round 1, cleint 14, epoch 2: epoch_accuracy 25.889756944444443, epoch_loss 2.162477042939928 \n",
      "Round 1, cleint 14, epoch 3: epoch_accuracy 25.9765625, epoch_loss 2.002375861008962 \n",
      "Round 1, cleint 14, epoch 4: epoch_accuracy 25.04340277777778, epoch_loss 1.9566184083620708 \n",
      "Round 1, cleint 14, epoch 5: epoch_accuracy 25.23871527777778, epoch_loss 1.9438162744045258 \n",
      "Round 1, cleint 15, epoch 1: epoch_accuracy 42.91294642857143, epoch_loss 2.2425267696380615 \n",
      "Round 1, cleint 15, epoch 2: epoch_accuracy 44.252232142857146, epoch_loss 2.136659707341875 \n",
      "Round 1, cleint 15, epoch 3: epoch_accuracy 44.419642857142854, epoch_loss 1.89692622423172 \n",
      "Round 1, cleint 15, epoch 4: epoch_accuracy 44.58705357142857, epoch_loss 1.5631842528070723 \n",
      "Round 1, cleint 15, epoch 5: epoch_accuracy 44.419642857142854, epoch_loss 1.5082036852836609 \n",
      "Round 1, cleint 16, epoch 1: epoch_accuracy 25.53453947368421, epoch_loss 2.258965718118768 \n",
      "Round 1, cleint 16, epoch 2: epoch_accuracy 37.21217105263158, epoch_loss 2.0774810690628853 \n",
      "Round 1, cleint 16, epoch 3: epoch_accuracy 37.99342105263158, epoch_loss 1.6678023087350946 \n",
      "Round 1, cleint 16, epoch 4: epoch_accuracy 39.02138157894737, epoch_loss 1.5449199551030208 \n",
      "Round 1, cleint 16, epoch 5: epoch_accuracy 39.84375, epoch_loss 1.523551300952309 \n",
      "Round 1, cleint 17, epoch 1: epoch_accuracy 22.1875, epoch_loss 2.2677255153656004 \n",
      "Round 1, cleint 17, epoch 2: epoch_accuracy 28.59375, epoch_loss 2.222170527776082 \n",
      "Round 1, cleint 17, epoch 3: epoch_accuracy 30.9375, epoch_loss 2.1585509618123373 \n",
      "Round 1, cleint 17, epoch 4: epoch_accuracy 31.197916666666668, epoch_loss 2.053593969345093 \n",
      "Round 1, cleint 17, epoch 5: epoch_accuracy 31.614583333333332, epoch_loss 1.9112910509109498 \n",
      "Round 1, cleint 18, epoch 1: epoch_accuracy 22.98869680851064, epoch_loss 2.240898862798163 \n",
      "Round 1, cleint 18, epoch 2: epoch_accuracy 28.05851063829787, epoch_loss 1.922367659020931 \n",
      "Round 1, cleint 18, epoch 3: epoch_accuracy 29.07247340425532, epoch_loss 1.7883396250136354 \n",
      "Round 1, cleint 18, epoch 4: epoch_accuracy 31.26662234042553, epoch_loss 1.7697583335511229 \n",
      "Round 1, cleint 18, epoch 5: epoch_accuracy 34.4248670212766, epoch_loss 1.735385316483518 \n",
      "Round 1, cleint 19, epoch 1: epoch_accuracy 12.734375, epoch_loss 2.293576216697693 \n",
      "Round 1, cleint 19, epoch 2: epoch_accuracy 38.515625, epoch_loss 2.259733533859253 \n",
      "Round 1, cleint 19, epoch 3: epoch_accuracy 40.546875, epoch_loss 2.2166477918624876 \n",
      "Round 1, cleint 19, epoch 4: epoch_accuracy 40.390625, epoch_loss 2.1604881286621094 \n",
      "Round 1, cleint 19, epoch 5: epoch_accuracy 40.390625, epoch_loss 2.0720904231071473 \n",
      "Round 1, cleint 20, epoch 1: epoch_accuracy 23.802083333333332, epoch_loss 2.264695183436076 \n",
      "Round 1, cleint 20, epoch 2: epoch_accuracy 33.020833333333336, epoch_loss 2.048214916388194 \n",
      "Round 1, cleint 20, epoch 3: epoch_accuracy 32.604166666666664, epoch_loss 1.7806058843930563 \n",
      "Round 1, cleint 20, epoch 4: epoch_accuracy 32.916666666666664, epoch_loss 1.736289127667745 \n",
      "Round 1, cleint 20, epoch 5: epoch_accuracy 32.864583333333336, epoch_loss 1.7187289555867513 \n",
      "Model's Round: 1, train accuracy of model: 35.174321616412314, train loss of model: 1.794476390448717 \n",
      "\n",
      "\n",
      "Model's Round: 1, test accuracy of model: 10.003993610223642, test loss of model: 1012.4429806840305 \n",
      "\n",
      "\n",
      "Round 1 completed\n",
      "Round 2, cleint 1, epoch 1: epoch_accuracy 14.270833333333334, epoch_loss 95.22193411191304 \n",
      "Round 2, cleint 1, epoch 2: epoch_accuracy 17.291666666666668, epoch_loss 2.2647939840952556 \n",
      "Round 2, cleint 1, epoch 3: epoch_accuracy 20.104166666666668, epoch_loss 2.25504682858785 \n",
      "Round 2, cleint 1, epoch 4: epoch_accuracy 20.9375, epoch_loss 2.2477872371673584 \n",
      "Round 2, cleint 1, epoch 5: epoch_accuracy 20.885416666666668, epoch_loss 2.2405618985493976 \n",
      "Round 2, cleint 2, epoch 1: epoch_accuracy 43.177083333333336, epoch_loss 112.44327067534128 \n",
      "Round 2, cleint 2, epoch 2: epoch_accuracy 46.354166666666664, epoch_loss 1.4903399149576824 \n",
      "Round 2, cleint 2, epoch 3: epoch_accuracy 46.614583333333336, epoch_loss 1.4750396013259888 \n",
      "Round 2, cleint 2, epoch 4: epoch_accuracy 45.885416666666664, epoch_loss 1.4887526909510294 \n",
      "Round 2, cleint 2, epoch 5: epoch_accuracy 45.833333333333336, epoch_loss 1.4851987997690836 \n",
      "Round 2, cleint 3, epoch 1: epoch_accuracy 12.67361111111111, epoch_loss 904.6822423228512 \n",
      "Round 2, cleint 3, epoch 2: epoch_accuracy 13.94675925925926, epoch_loss 2.3688485710709184 \n",
      "Round 2, cleint 3, epoch 3: epoch_accuracy 21.730324074074073, epoch_loss 2.304636999412819 \n",
      "Round 2, cleint 3, epoch 4: epoch_accuracy 23.78472222222222, epoch_loss 2.236801818565086 \n",
      "Round 2, cleint 3, epoch 5: epoch_accuracy 24.16087962962963, epoch_loss 2.2130090219003185 \n",
      "Round 2, cleint 4, epoch 1: epoch_accuracy 32.760416666666664, epoch_loss 113.2798881371816 \n",
      "Round 2, cleint 4, epoch 2: epoch_accuracy 34.583333333333336, epoch_loss 2.1011988719304404 \n",
      "Round 2, cleint 4, epoch 3: epoch_accuracy 34.479166666666664, epoch_loss 2.0922489802042645 \n",
      "Round 2, cleint 4, epoch 4: epoch_accuracy 34.53125, epoch_loss 2.080362590154012 \n",
      "Round 2, cleint 4, epoch 5: epoch_accuracy 34.479166666666664, epoch_loss 2.074410072962443 \n",
      "Round 2, cleint 5, epoch 1: epoch_accuracy 11.41826923076923, epoch_loss nan \n",
      "Round 2, cleint 5, epoch 2: epoch_accuracy 0.7512019230769231, epoch_loss nan \n",
      "Round 2, cleint 5, epoch 3: epoch_accuracy 0.7512019230769231, epoch_loss nan \n",
      "Round 2, cleint 5, epoch 4: epoch_accuracy 0.7512019230769231, epoch_loss nan \n",
      "Round 2, cleint 5, epoch 5: epoch_accuracy 0.7512019230769231, epoch_loss nan \n",
      "Round 2, cleint 6, epoch 1: epoch_accuracy 23.828125, epoch_loss 73.55633826255799 \n",
      "Round 2, cleint 6, epoch 2: epoch_accuracy 24.375, epoch_loss 2.221117651462555 \n",
      "Round 2, cleint 6, epoch 3: epoch_accuracy 24.1796875, epoch_loss 2.193148684501648 \n",
      "Round 2, cleint 6, epoch 4: epoch_accuracy 24.0234375, epoch_loss 2.173807156085968 \n",
      "Round 2, cleint 6, epoch 5: epoch_accuracy 24.0234375, epoch_loss 2.154365670681 \n",
      "Round 2, cleint 7, epoch 1: epoch_accuracy 49.59077380952381, epoch_loss 82.07593228703453 \n",
      "Round 2, cleint 7, epoch 2: epoch_accuracy 51.339285714285715, epoch_loss 1.8238991158349174 \n",
      "Round 2, cleint 7, epoch 3: epoch_accuracy 51.413690476190474, epoch_loss 1.8018364508946736 \n",
      "Round 2, cleint 7, epoch 4: epoch_accuracy 51.26488095238095, epoch_loss 1.7943009876069569 \n",
      "Round 2, cleint 7, epoch 5: epoch_accuracy 51.19047619047619, epoch_loss 1.7781519833065214 \n",
      "Round 2, cleint 8, epoch 1: epoch_accuracy 48.876953125, epoch_loss 103.06526409834623 \n",
      "Round 2, cleint 8, epoch 2: epoch_accuracy 51.3671875, epoch_loss 1.7310380414128304 \n",
      "Round 2, cleint 8, epoch 3: epoch_accuracy 51.611328125, epoch_loss 1.7210845053195953 \n",
      "Round 2, cleint 8, epoch 4: epoch_accuracy 51.5625, epoch_loss 1.7139437422156334 \n",
      "Round 2, cleint 8, epoch 5: epoch_accuracy 51.46484375, epoch_loss 1.7037452384829521 \n",
      "Round 2, cleint 9, epoch 1: epoch_accuracy 19.301470588235293, epoch_loss 77.57639509088852 \n",
      "Round 2, cleint 9, epoch 2: epoch_accuracy 19.80698529411765, epoch_loss 2.07328358818503 \n",
      "Round 2, cleint 9, epoch 3: epoch_accuracy 19.11764705882353, epoch_loss 2.0452772238675285 \n",
      "Round 2, cleint 9, epoch 4: epoch_accuracy 19.485294117647058, epoch_loss 2.0263233815922455 \n",
      "Round 2, cleint 9, epoch 5: epoch_accuracy 19.94485294117647, epoch_loss 2.0111283835242775 \n",
      "Round 2, cleint 10, epoch 1: epoch_accuracy 31.640625, epoch_loss 102.54015150666237 \n",
      "Round 2, cleint 10, epoch 2: epoch_accuracy 39.127604166666664, epoch_loss 1.9850665430227916 \n",
      "Round 2, cleint 10, epoch 3: epoch_accuracy 39.0625, epoch_loss 1.9736071924368541 \n",
      "Round 2, cleint 10, epoch 4: epoch_accuracy 38.997395833333336, epoch_loss 1.9596850474675496 \n",
      "Round 2, cleint 10, epoch 5: epoch_accuracy 39.0625, epoch_loss 1.9633102118968964 \n",
      "Round 2, cleint 11, epoch 1: epoch_accuracy 23.125, epoch_loss 336.02857065200806 \n",
      "Round 2, cleint 11, epoch 2: epoch_accuracy 26.09375, epoch_loss 2.0177098512649536 \n",
      "Round 2, cleint 11, epoch 3: epoch_accuracy 30.3125, epoch_loss 2.013183927536011 \n",
      "Round 2, cleint 11, epoch 4: epoch_accuracy 30.625, epoch_loss 1.9985196828842162 \n",
      "Round 2, cleint 11, epoch 5: epoch_accuracy 30.3125, epoch_loss 1.9872631311416626 \n",
      "Round 2, cleint 12, epoch 1: epoch_accuracy 39.91477272727273, epoch_loss 153.86635639450768 \n",
      "Round 2, cleint 12, epoch 2: epoch_accuracy 44.10511363636363, epoch_loss 1.4765719283710828 \n",
      "Round 2, cleint 12, epoch 3: epoch_accuracy 44.03409090909091, epoch_loss 1.493675871328874 \n",
      "Round 2, cleint 12, epoch 4: epoch_accuracy 43.96306818181818, epoch_loss 1.4760597510771318 \n",
      "Round 2, cleint 12, epoch 5: epoch_accuracy 43.39488636363637, epoch_loss 1.4842997139150447 \n",
      "Round 2, cleint 13, epoch 1: epoch_accuracy 14.6875, epoch_loss 126.899174618721 \n",
      "Round 2, cleint 13, epoch 2: epoch_accuracy 15.15625, epoch_loss 2.3018006086349487 \n",
      "Round 2, cleint 13, epoch 3: epoch_accuracy 15.546875, epoch_loss 2.2950478315353395 \n",
      "Round 2, cleint 13, epoch 4: epoch_accuracy 15.390625, epoch_loss 2.2912384986877443 \n",
      "Round 2, cleint 13, epoch 5: epoch_accuracy 15.46875, epoch_loss 2.2871158123016357 \n",
      "Round 2, cleint 14, epoch 1: epoch_accuracy 25.672743055555557, epoch_loss 42.22568478849199 \n",
      "Round 2, cleint 14, epoch 2: epoch_accuracy 26.063368055555557, epoch_loss 2.261132743623522 \n",
      "Round 2, cleint 14, epoch 3: epoch_accuracy 25.78125, epoch_loss 2.2401645713382297 \n",
      "Round 2, cleint 14, epoch 4: epoch_accuracy 25.9765625, epoch_loss 2.222727656364441 \n",
      "Round 2, cleint 14, epoch 5: epoch_accuracy 25.846354166666668, epoch_loss 2.2066312895880804 \n",
      "Round 2, cleint 15, epoch 1: epoch_accuracy 41.238839285714285, epoch_loss 132.2760890977723 \n",
      "Round 2, cleint 15, epoch 2: epoch_accuracy 44.252232142857146, epoch_loss 1.5238744361060006 \n",
      "Round 2, cleint 15, epoch 3: epoch_accuracy 44.53125, epoch_loss 1.5244835359709603 \n",
      "Round 2, cleint 15, epoch 4: epoch_accuracy 44.810267857142854, epoch_loss 1.510866914476667 \n",
      "Round 2, cleint 15, epoch 5: epoch_accuracy 44.363839285714285, epoch_loss 1.5174619214875358 \n",
      "Round 2, cleint 16, epoch 1: epoch_accuracy 36.55427631578947, epoch_loss 96.61805108973854 \n",
      "Round 2, cleint 16, epoch 2: epoch_accuracy 37.25328947368421, epoch_loss 1.9475531578063965 \n",
      "Round 2, cleint 16, epoch 3: epoch_accuracy 37.41776315789474, epoch_loss 1.9258262985631038 \n",
      "Round 2, cleint 16, epoch 4: epoch_accuracy 37.33552631578947, epoch_loss 1.9068294010664288 \n",
      "Round 2, cleint 16, epoch 5: epoch_accuracy 37.088815789473685, epoch_loss 1.8876029253005981 \n",
      "Round 2, cleint 17, epoch 1: epoch_accuracy 29.739583333333332, epoch_loss 111.43860179583231 \n",
      "Round 2, cleint 17, epoch 2: epoch_accuracy 32.03125, epoch_loss 2.1106118679046633 \n",
      "Round 2, cleint 17, epoch 3: epoch_accuracy 31.666666666666668, epoch_loss 2.0928951581319173 \n",
      "Round 2, cleint 17, epoch 4: epoch_accuracy 31.614583333333332, epoch_loss 2.0834258794784546 \n",
      "Round 2, cleint 17, epoch 5: epoch_accuracy 31.510416666666668, epoch_loss 2.0744877179463703 \n",
      "Round 2, cleint 18, epoch 1: epoch_accuracy 27.94215425531915, epoch_loss 28.55099182940544 \n",
      "Round 2, cleint 18, epoch 2: epoch_accuracy 28.14162234042553, epoch_loss 2.10126323395587 \n",
      "Round 2, cleint 18, epoch 3: epoch_accuracy 27.94215425531915, epoch_loss 2.069378474925427 \n",
      "Round 2, cleint 18, epoch 4: epoch_accuracy 27.925531914893618, epoch_loss 2.0522814989089966 \n",
      "Round 2, cleint 18, epoch 5: epoch_accuracy 27.95877659574468, epoch_loss 2.0340418840976473 \n",
      "Round 2, cleint 19, epoch 1: epoch_accuracy 37.1875, epoch_loss 186.0001852273941 \n",
      "Round 2, cleint 19, epoch 2: epoch_accuracy 40.46875, epoch_loss 2.0407699823379515 \n",
      "Round 2, cleint 19, epoch 3: epoch_accuracy 39.84375, epoch_loss 2.0361639142036436 \n",
      "Round 2, cleint 19, epoch 4: epoch_accuracy 40.546875, epoch_loss 2.0114624977111815 \n",
      "Round 2, cleint 19, epoch 5: epoch_accuracy 39.765625, epoch_loss 2.0259810209274294 \n",
      "Round 2, cleint 20, epoch 1: epoch_accuracy 19.973958333333332, epoch_loss nan \n",
      "Round 2, cleint 20, epoch 2: epoch_accuracy 17.1875, epoch_loss nan \n",
      "Round 2, cleint 20, epoch 3: epoch_accuracy 17.213541666666668, epoch_loss nan \n",
      "Round 2, cleint 20, epoch 4: epoch_accuracy 17.109375, epoch_loss nan \n",
      "Round 2, cleint 20, epoch 5: epoch_accuracy 17.213541666666668, epoch_loss nan \n",
      "Model's Round: 2, train accuracy of model: 31.23598070677975, train loss of model: nan \n",
      "\n",
      "\n",
      "Model's Round: 2, test accuracy of model: 10.003993610223642, test loss of model: nan \n",
      "\n",
      "\n",
      "Round 2 completed\n",
      "Round 3, cleint 1, epoch 1: epoch_accuracy 4.791666666666667, epoch_loss nan \n",
      "Round 3, cleint 1, epoch 2: epoch_accuracy 4.84375, epoch_loss nan \n",
      "Round 3, cleint 1, epoch 3: epoch_accuracy 4.84375, epoch_loss nan \n",
      "Round 3, cleint 1, epoch 4: epoch_accuracy 4.84375, epoch_loss nan \n",
      "Round 3, cleint 1, epoch 5: epoch_accuracy 4.84375, epoch_loss nan \n",
      "Round 3, cleint 2, epoch 1: epoch_accuracy 1.9791666666666667, epoch_loss nan \n",
      "Round 3, cleint 2, epoch 2: epoch_accuracy 1.9270833333333333, epoch_loss nan \n",
      "Round 3, cleint 2, epoch 3: epoch_accuracy 1.9791666666666667, epoch_loss nan \n",
      "Round 3, cleint 2, epoch 4: epoch_accuracy 1.9791666666666667, epoch_loss nan \n",
      "Round 3, cleint 2, epoch 5: epoch_accuracy 1.9791666666666667, epoch_loss nan \n",
      "Round 3, cleint 3, epoch 1: epoch_accuracy 23.87152777777778, epoch_loss nan \n",
      "Round 3, cleint 3, epoch 2: epoch_accuracy 24.016203703703702, epoch_loss nan \n",
      "Round 3, cleint 3, epoch 3: epoch_accuracy 23.78472222222222, epoch_loss nan \n",
      "Round 3, cleint 3, epoch 4: epoch_accuracy 24.04513888888889, epoch_loss nan \n",
      "Round 3, cleint 3, epoch 5: epoch_accuracy 23.755787037037038, epoch_loss nan \n",
      "Round 3, cleint 4, epoch 1: epoch_accuracy 18.958333333333332, epoch_loss nan \n",
      "Round 3, cleint 4, epoch 2: epoch_accuracy 18.854166666666668, epoch_loss nan \n",
      "Round 3, cleint 4, epoch 3: epoch_accuracy 18.90625, epoch_loss nan \n",
      "Round 3, cleint 4, epoch 4: epoch_accuracy 18.854166666666668, epoch_loss nan \n",
      "Round 3, cleint 4, epoch 5: epoch_accuracy 18.854166666666668, epoch_loss nan \n",
      "Round 3, cleint 5, epoch 1: epoch_accuracy 0.7512019230769231, epoch_loss nan \n",
      "Round 3, cleint 5, epoch 2: epoch_accuracy 0.7512019230769231, epoch_loss nan \n",
      "Round 3, cleint 5, epoch 3: epoch_accuracy 0.7512019230769231, epoch_loss nan \n",
      "Round 3, cleint 5, epoch 4: epoch_accuracy 0.7512019230769231, epoch_loss nan \n",
      "Round 3, cleint 5, epoch 5: epoch_accuracy 0.7512019230769231, epoch_loss nan \n",
      "Round 3, cleint 6, epoch 1: epoch_accuracy 8.9453125, epoch_loss nan \n",
      "Round 3, cleint 6, epoch 2: epoch_accuracy 9.1796875, epoch_loss nan \n",
      "Round 3, cleint 6, epoch 3: epoch_accuracy 8.8671875, epoch_loss nan \n",
      "Round 3, cleint 6, epoch 4: epoch_accuracy 8.9453125, epoch_loss nan \n",
      "Round 3, cleint 6, epoch 5: epoch_accuracy 8.984375, epoch_loss nan \n",
      "Round 3, cleint 7, epoch 1: epoch_accuracy 5.877976190476191, epoch_loss nan \n",
      "Round 3, cleint 7, epoch 2: epoch_accuracy 5.840773809523809, epoch_loss nan \n",
      "Round 3, cleint 7, epoch 3: epoch_accuracy 5.877976190476191, epoch_loss nan \n",
      "Round 3, cleint 7, epoch 4: epoch_accuracy 5.877976190476191, epoch_loss nan \n",
      "Round 3, cleint 7, epoch 5: epoch_accuracy 5.840773809523809, epoch_loss nan \n",
      "Round 3, cleint 8, epoch 1: epoch_accuracy 10.546875, epoch_loss nan \n",
      "Round 3, cleint 8, epoch 2: epoch_accuracy 10.498046875, epoch_loss nan \n",
      "Round 3, cleint 8, epoch 3: epoch_accuracy 10.44921875, epoch_loss nan \n",
      "Round 3, cleint 8, epoch 4: epoch_accuracy 10.546875, epoch_loss nan \n",
      "Round 3, cleint 8, epoch 5: epoch_accuracy 10.44921875, epoch_loss nan \n",
      "Round 3, cleint 9, epoch 1: epoch_accuracy 0.0, epoch_loss nan \n",
      "Round 3, cleint 9, epoch 2: epoch_accuracy 0.0, epoch_loss nan \n",
      "Round 3, cleint 9, epoch 3: epoch_accuracy 0.0, epoch_loss nan \n",
      "Round 3, cleint 9, epoch 4: epoch_accuracy 0.0, epoch_loss nan \n",
      "Round 3, cleint 9, epoch 5: epoch_accuracy 0.0, epoch_loss nan \n",
      "Round 3, cleint 10, epoch 1: epoch_accuracy 39.127604166666664, epoch_loss nan \n",
      "Round 3, cleint 10, epoch 2: epoch_accuracy 38.932291666666664, epoch_loss nan \n",
      "Round 3, cleint 10, epoch 3: epoch_accuracy 39.127604166666664, epoch_loss nan \n",
      "Round 3, cleint 10, epoch 4: epoch_accuracy 38.932291666666664, epoch_loss nan \n",
      "Round 3, cleint 10, epoch 5: epoch_accuracy 38.8671875, epoch_loss nan \n",
      "Round 3, cleint 11, epoch 1: epoch_accuracy 0.3125, epoch_loss nan \n",
      "Round 3, cleint 11, epoch 2: epoch_accuracy 0.3125, epoch_loss nan \n",
      "Round 3, cleint 11, epoch 3: epoch_accuracy 0.3125, epoch_loss nan \n",
      "Round 3, cleint 11, epoch 4: epoch_accuracy 0.3125, epoch_loss nan \n",
      "Round 3, cleint 11, epoch 5: epoch_accuracy 0.3125, epoch_loss nan \n",
      "Round 3, cleint 12, epoch 1: epoch_accuracy 0.5681818181818182, epoch_loss nan \n",
      "Round 3, cleint 12, epoch 2: epoch_accuracy 0.5681818181818182, epoch_loss nan \n",
      "Round 3, cleint 12, epoch 3: epoch_accuracy 0.7102272727272727, epoch_loss nan \n",
      "Round 3, cleint 12, epoch 4: epoch_accuracy 0.6392045454545454, epoch_loss nan \n",
      "Round 3, cleint 12, epoch 5: epoch_accuracy 0.7102272727272727, epoch_loss nan \n",
      "Round 3, cleint 13, epoch 1: epoch_accuracy 20.703125, epoch_loss nan \n",
      "Round 3, cleint 13, epoch 2: epoch_accuracy 21.09375, epoch_loss nan \n",
      "Round 3, cleint 13, epoch 3: epoch_accuracy 20.9375, epoch_loss nan \n",
      "Round 3, cleint 13, epoch 4: epoch_accuracy 21.09375, epoch_loss nan \n",
      "Round 3, cleint 13, epoch 5: epoch_accuracy 20.625, epoch_loss nan \n",
      "Round 3, cleint 14, epoch 1: epoch_accuracy 12.67361111111111, epoch_loss nan \n",
      "Round 3, cleint 14, epoch 2: epoch_accuracy 12.52170138888889, epoch_loss nan \n",
      "Round 3, cleint 14, epoch 3: epoch_accuracy 12.651909722222221, epoch_loss nan \n",
      "Round 3, cleint 14, epoch 4: epoch_accuracy 12.651909722222221, epoch_loss nan \n",
      "Round 3, cleint 14, epoch 5: epoch_accuracy 12.608506944444445, epoch_loss nan \n",
      "Round 3, cleint 15, epoch 1: epoch_accuracy 1.5625, epoch_loss nan \n",
      "Round 3, cleint 15, epoch 2: epoch_accuracy 1.6183035714285714, epoch_loss nan \n",
      "Round 3, cleint 15, epoch 3: epoch_accuracy 1.5625, epoch_loss nan \n",
      "Round 3, cleint 15, epoch 4: epoch_accuracy 1.6183035714285714, epoch_loss nan \n",
      "Round 3, cleint 15, epoch 5: epoch_accuracy 1.6741071428571428, epoch_loss nan \n",
      "Round 3, cleint 16, epoch 1: epoch_accuracy 0.0, epoch_loss nan \n",
      "Round 3, cleint 16, epoch 2: epoch_accuracy 0.0, epoch_loss nan \n",
      "Round 3, cleint 16, epoch 3: epoch_accuracy 0.0, epoch_loss nan \n",
      "Round 3, cleint 16, epoch 4: epoch_accuracy 0.0, epoch_loss nan \n",
      "Round 3, cleint 16, epoch 5: epoch_accuracy 0.0, epoch_loss nan \n",
      "Round 3, cleint 17, epoch 1: epoch_accuracy 3.8020833333333335, epoch_loss nan \n",
      "Round 3, cleint 17, epoch 2: epoch_accuracy 3.8020833333333335, epoch_loss nan \n",
      "Round 3, cleint 17, epoch 3: epoch_accuracy 3.8541666666666665, epoch_loss nan \n",
      "Round 3, cleint 17, epoch 4: epoch_accuracy 3.8020833333333335, epoch_loss nan \n",
      "Round 3, cleint 17, epoch 5: epoch_accuracy 3.9583333333333335, epoch_loss nan \n",
      "Round 3, cleint 18, epoch 1: epoch_accuracy 10.987367021276595, epoch_loss nan \n",
      "Round 3, cleint 18, epoch 2: epoch_accuracy 11.02061170212766, epoch_loss nan \n",
      "Round 3, cleint 18, epoch 3: epoch_accuracy 11.053856382978724, epoch_loss nan \n",
      "Round 3, cleint 18, epoch 4: epoch_accuracy 10.871010638297872, epoch_loss nan \n",
      "Round 3, cleint 18, epoch 5: epoch_accuracy 10.954122340425531, epoch_loss nan \n",
      "Round 3, cleint 19, epoch 1: epoch_accuracy 5.234375, epoch_loss nan \n",
      "Round 3, cleint 19, epoch 2: epoch_accuracy 5.3125, epoch_loss nan \n",
      "Round 3, cleint 19, epoch 3: epoch_accuracy 5.078125, epoch_loss nan \n",
      "Round 3, cleint 19, epoch 4: epoch_accuracy 5.234375, epoch_loss nan \n",
      "Round 3, cleint 19, epoch 5: epoch_accuracy 5.234375, epoch_loss nan \n",
      "Round 3, cleint 20, epoch 1: epoch_accuracy 17.265625, epoch_loss nan \n",
      "Round 3, cleint 20, epoch 2: epoch_accuracy 17.213541666666668, epoch_loss nan \n",
      "Round 3, cleint 20, epoch 3: epoch_accuracy 17.161458333333332, epoch_loss nan \n",
      "Round 3, cleint 20, epoch 4: epoch_accuracy 17.239583333333332, epoch_loss nan \n",
      "Round 3, cleint 20, epoch 5: epoch_accuracy 17.1875, epoch_loss nan \n",
      "Model's Round: 3, train accuracy of model: 9.379514969337942, train loss of model: nan \n",
      "\n",
      "\n",
      "Model's Round: 3, test accuracy of model: 10.003993610223642, test loss of model: nan \n",
      "\n",
      "\n",
      "Round 3 completed\n",
      "Round 4, cleint 1, epoch 1: epoch_accuracy 4.84375, epoch_loss nan \n",
      "Round 4, cleint 1, epoch 2: epoch_accuracy 4.791666666666667, epoch_loss nan \n",
      "Round 4, cleint 1, epoch 3: epoch_accuracy 4.84375, epoch_loss nan \n",
      "Round 4, cleint 1, epoch 4: epoch_accuracy 4.84375, epoch_loss nan \n",
      "Round 4, cleint 1, epoch 5: epoch_accuracy 4.635416666666667, epoch_loss nan \n",
      "Round 4, cleint 2, epoch 1: epoch_accuracy 1.9791666666666667, epoch_loss nan \n",
      "Round 4, cleint 2, epoch 2: epoch_accuracy 1.875, epoch_loss nan \n",
      "Round 4, cleint 2, epoch 3: epoch_accuracy 1.9791666666666667, epoch_loss nan \n",
      "Round 4, cleint 2, epoch 4: epoch_accuracy 1.875, epoch_loss nan \n",
      "Round 4, cleint 2, epoch 5: epoch_accuracy 1.875, epoch_loss nan \n",
      "Round 4, cleint 3, epoch 1: epoch_accuracy 24.04513888888889, epoch_loss nan \n",
      "Round 4, cleint 3, epoch 2: epoch_accuracy 24.04513888888889, epoch_loss nan \n",
      "Round 4, cleint 3, epoch 3: epoch_accuracy 23.900462962962962, epoch_loss nan \n",
      "Round 4, cleint 3, epoch 4: epoch_accuracy 23.84259259259259, epoch_loss nan \n",
      "Round 4, cleint 3, epoch 5: epoch_accuracy 23.87152777777778, epoch_loss nan \n",
      "Round 4, cleint 4, epoch 1: epoch_accuracy 19.010416666666668, epoch_loss nan \n",
      "Round 4, cleint 4, epoch 2: epoch_accuracy 18.90625, epoch_loss nan \n",
      "Round 4, cleint 4, epoch 3: epoch_accuracy 19.010416666666668, epoch_loss nan \n",
      "Round 4, cleint 4, epoch 4: epoch_accuracy 18.802083333333332, epoch_loss nan \n",
      "Round 4, cleint 4, epoch 5: epoch_accuracy 18.75, epoch_loss nan \n",
      "Round 4, cleint 5, epoch 1: epoch_accuracy 0.7512019230769231, epoch_loss nan \n",
      "Round 4, cleint 5, epoch 2: epoch_accuracy 0.7512019230769231, epoch_loss nan \n",
      "Round 4, cleint 5, epoch 3: epoch_accuracy 0.7211538461538461, epoch_loss nan \n",
      "Round 4, cleint 5, epoch 4: epoch_accuracy 0.7512019230769231, epoch_loss nan \n",
      "Round 4, cleint 5, epoch 5: epoch_accuracy 0.7512019230769231, epoch_loss nan \n",
      "Round 4, cleint 6, epoch 1: epoch_accuracy 9.1796875, epoch_loss nan \n",
      "Round 4, cleint 6, epoch 2: epoch_accuracy 9.140625, epoch_loss nan \n",
      "Round 4, cleint 6, epoch 3: epoch_accuracy 8.828125, epoch_loss nan \n",
      "Round 4, cleint 6, epoch 4: epoch_accuracy 9.1796875, epoch_loss nan \n",
      "Round 4, cleint 6, epoch 5: epoch_accuracy 8.90625, epoch_loss nan \n",
      "Round 4, cleint 7, epoch 1: epoch_accuracy 5.915178571428571, epoch_loss nan \n",
      "Round 4, cleint 7, epoch 2: epoch_accuracy 5.840773809523809, epoch_loss nan \n",
      "Round 4, cleint 7, epoch 3: epoch_accuracy 5.803571428571429, epoch_loss nan \n",
      "Round 4, cleint 7, epoch 4: epoch_accuracy 5.803571428571429, epoch_loss nan \n",
      "Round 4, cleint 7, epoch 5: epoch_accuracy 5.877976190476191, epoch_loss nan \n",
      "Round 4, cleint 8, epoch 1: epoch_accuracy 10.44921875, epoch_loss nan \n",
      "Round 4, cleint 8, epoch 2: epoch_accuracy 10.498046875, epoch_loss nan \n",
      "Round 4, cleint 8, epoch 3: epoch_accuracy 10.498046875, epoch_loss nan \n",
      "Round 4, cleint 8, epoch 4: epoch_accuracy 10.546875, epoch_loss nan \n",
      "Round 4, cleint 8, epoch 5: epoch_accuracy 10.595703125, epoch_loss nan \n",
      "Round 4, cleint 9, epoch 1: epoch_accuracy 0.0, epoch_loss nan \n",
      "Round 4, cleint 9, epoch 2: epoch_accuracy 0.0, epoch_loss nan \n",
      "Round 4, cleint 9, epoch 3: epoch_accuracy 0.0, epoch_loss nan \n",
      "Round 4, cleint 9, epoch 4: epoch_accuracy 0.0, epoch_loss nan \n",
      "Round 4, cleint 9, epoch 5: epoch_accuracy 0.0, epoch_loss nan \n",
      "Round 4, cleint 10, epoch 1: epoch_accuracy 39.0625, epoch_loss nan \n",
      "Round 4, cleint 10, epoch 2: epoch_accuracy 38.997395833333336, epoch_loss nan \n",
      "Round 4, cleint 10, epoch 3: epoch_accuracy 39.127604166666664, epoch_loss nan \n",
      "Round 4, cleint 10, epoch 4: epoch_accuracy 38.997395833333336, epoch_loss nan \n",
      "Round 4, cleint 10, epoch 5: epoch_accuracy 38.932291666666664, epoch_loss nan \n",
      "Round 4, cleint 11, epoch 1: epoch_accuracy 0.3125, epoch_loss nan \n",
      "Round 4, cleint 11, epoch 2: epoch_accuracy 0.15625, epoch_loss nan \n",
      "Round 4, cleint 11, epoch 3: epoch_accuracy 0.3125, epoch_loss nan \n",
      "Round 4, cleint 11, epoch 4: epoch_accuracy 0.3125, epoch_loss nan \n",
      "Round 4, cleint 11, epoch 5: epoch_accuracy 0.15625, epoch_loss nan \n",
      "Round 4, cleint 12, epoch 1: epoch_accuracy 0.5681818181818182, epoch_loss nan \n",
      "Round 4, cleint 12, epoch 2: epoch_accuracy 0.7102272727272727, epoch_loss nan \n",
      "Round 4, cleint 12, epoch 3: epoch_accuracy 0.6392045454545454, epoch_loss nan \n",
      "Round 4, cleint 12, epoch 4: epoch_accuracy 0.5681818181818182, epoch_loss nan \n",
      "Round 4, cleint 12, epoch 5: epoch_accuracy 0.7102272727272727, epoch_loss nan \n",
      "Round 4, cleint 13, epoch 1: epoch_accuracy 20.390625, epoch_loss nan \n",
      "Round 4, cleint 13, epoch 2: epoch_accuracy 20.46875, epoch_loss nan \n",
      "Round 4, cleint 13, epoch 3: epoch_accuracy 20.78125, epoch_loss nan \n",
      "Round 4, cleint 13, epoch 4: epoch_accuracy 20.9375, epoch_loss nan \n",
      "Round 4, cleint 13, epoch 5: epoch_accuracy 21.015625, epoch_loss nan \n",
      "Round 4, cleint 14, epoch 1: epoch_accuracy 12.71701388888889, epoch_loss nan \n",
      "Round 4, cleint 14, epoch 2: epoch_accuracy 12.6953125, epoch_loss nan \n",
      "Round 4, cleint 14, epoch 3: epoch_accuracy 12.630208333333334, epoch_loss nan \n",
      "Round 4, cleint 14, epoch 4: epoch_accuracy 12.52170138888889, epoch_loss nan \n",
      "Round 4, cleint 14, epoch 5: epoch_accuracy 12.608506944444445, epoch_loss nan \n",
      "Round 4, cleint 15, epoch 1: epoch_accuracy 1.5625, epoch_loss nan \n",
      "Round 4, cleint 15, epoch 2: epoch_accuracy 1.5066964285714286, epoch_loss nan \n",
      "Round 4, cleint 15, epoch 3: epoch_accuracy 1.6741071428571428, epoch_loss nan \n",
      "Round 4, cleint 15, epoch 4: epoch_accuracy 1.6741071428571428, epoch_loss nan \n",
      "Round 4, cleint 15, epoch 5: epoch_accuracy 1.5066964285714286, epoch_loss nan \n",
      "Round 4, cleint 16, epoch 1: epoch_accuracy 0.0, epoch_loss nan \n",
      "Round 4, cleint 16, epoch 2: epoch_accuracy 0.0, epoch_loss nan \n",
      "Round 4, cleint 16, epoch 3: epoch_accuracy 0.0, epoch_loss nan \n",
      "Round 4, cleint 16, epoch 4: epoch_accuracy 0.0, epoch_loss nan \n",
      "Round 4, cleint 16, epoch 5: epoch_accuracy 0.0, epoch_loss nan \n",
      "Round 4, cleint 17, epoch 1: epoch_accuracy 3.9583333333333335, epoch_loss nan \n",
      "Round 4, cleint 17, epoch 2: epoch_accuracy 3.75, epoch_loss nan \n",
      "Round 4, cleint 17, epoch 3: epoch_accuracy 3.8020833333333335, epoch_loss nan \n",
      "Round 4, cleint 17, epoch 4: epoch_accuracy 3.9583333333333335, epoch_loss nan \n",
      "Round 4, cleint 17, epoch 5: epoch_accuracy 3.6458333333333335, epoch_loss nan \n"
     ]
    }
   ],
   "source": [
    "federated_learning(initial_weights, client_datasets, client_no, participating_client, round_no, epochs, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a91d029",
   "metadata": {},
   "source": [
    "# Reuslts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fa4db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder and file name\n",
    "folder_name = f\"{method}_{opti}_{learning_rate}_{participating_client}_{client_no}_{distributions}_{alpha}\"  # Folder where the Excel file will be saved\n",
    "file_name = \"round_results.xlsx\"\n",
    "\n",
    "\n",
    "# Check if the folder exists, if not, create it\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Full path where the Excel file will be saved\n",
    "file_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "round_results.to_excel(file_path, index=False)\n",
    "\n",
    "print(\"DataFrame successfully written for round results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc1e6ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the folder and file name\n",
    "folder_name =  f\"{method}_{opti}_{learning_rate}_{participating_client}_{client_no}_{distributions}_{alpha}\"   # Folder where the Excel file will be saved\n",
    "file_name = \"epoch_results.xlsx\"\n",
    "\n",
    "# Check if the folder exists, if not, create it\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Full path where the Excel file will be saved\n",
    "file_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "epoch_results.to_excel(file_path, index=False)\n",
    "\n",
    "print(\"DataFrame successfully written for epoch results.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7a26882b",
    "f40fd883",
    "bef90546"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
