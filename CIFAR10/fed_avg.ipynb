{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a26882b",
   "metadata": {
    "id": "7a26882b"
   },
   "source": [
    "<H1>Import Libraries</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "473cd34e",
   "metadata": {
    "id": "473cd34e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision.datasets import CIFAR10\n",
    "from collections import Counter\n",
    "import random\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from torch import Tensor\n",
    "from typing import Type\n",
    "import torchvision.models as models\n",
    "from collections import Counter\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Set font family for plots\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b21b0f-99de-4780-9213-7a44331d8043",
   "metadata": {},
   "source": [
    "<H1>Load Dataset</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed95d81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(), #This transformation converts a PIL (Python Imaging Library) Image or numpy.ndarray (with shape (H x W x C) in the range [0, 255]) into a PyTorch tensor of shape (C x H x W) in the range [0.0, 1.0]. It essentially rearranges the dimensions of the image data and scales it to a float value between 0 and 1.\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# ])\n",
    "# train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "738ce049",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "738ce049",
    "outputId": "16e7254f-7275-464e-e7cb-552f9fb6ce34"
   },
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(), #This transformation converts a PIL (Python Imaging Library) Image or numpy.ndarray (with shape (H x W x C) in the range [0, 255]) into a PyTorch tensor of shape (C x H x W) in the range [0.0, 1.0]. It essentially rearranges the dimensions of the image data and scales it to a float value between 0 and 1.\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# ])\n",
    "# train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "subset_dataset = torch.load('val_dataset.pth',weights_only=False)\n",
    "remaining_dataset = torch.load('test_dataset.pth',weights_only=False)\n",
    "\n",
    "# Create DataLoaders\n",
    "val_loader = DataLoader(subset_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(remaining_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df656741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "# # full_test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# # # Initialize a Counter to store the distribution\n",
    "# # class_counts = Counter()\n",
    "\n",
    "# # # Count the labels in the test dataset\n",
    "# # for _, label in test_dataset:\n",
    "# #     class_counts[label] += 1\n",
    "\n",
    "# # # Print the distribution of the test dataset\n",
    "# # print(\"Class distribution in CIFAR-10 test dataset:\")\n",
    "# # for class_id, count in class_counts.items():\n",
    "# #     print(f\"Class {class_id}: {count} images\")\n",
    "\n",
    "\n",
    "\n",
    "# # # Initialize a dictionary to hold indices for each class, with a maximum of 250 per class\n",
    "# # class_counts = {i: 0 for i in range(10)}\n",
    "# # class_indices = {i: [] for i in range(10)}\n",
    "\n",
    "# # # Iterate over the dataset and collect indices\n",
    "# # for idx, (data, target) in enumerate(full_test_loader.dataset):\n",
    "# #     label = target\n",
    "# #     if class_counts[label] < 250:\n",
    "# #         class_indices[label].append(idx)\n",
    "# #         class_counts[label] += 1\n",
    "# #     # Break once we've collected 250 indices for each class\n",
    "# #     if all(count == 250 for count in class_counts.values()):\n",
    "# #         break\n",
    "\n",
    "# # # Get the selected and remaining indices\n",
    "# # selected_indices = {idx for indices in class_indices.values() for idx in indices}\n",
    "# # remaining_indices = [idx for idx in range(len(test_dataset)) if idx not in selected_indices]\n",
    "\n",
    "# # # Create subsets for the selected and remaining images\n",
    "# # subset_dataset = Subset(test_dataset, list(selected_indices))\n",
    "# # remaining_dataset = Subset(test_dataset, remaining_indices)\n",
    "\n",
    "\n",
    "# # # Save the subset and remaining datasets\n",
    "# # torch.save(subset_dataset, 'val_dataset.pth')\n",
    "# # torch.save(remaining_dataset, 'test_dataset.pth')\n",
    "\n",
    "# # Load the datasets\n",
    "# subset_dataset = torch.load('subset_dataset.pth')\n",
    "# remaining_dataset = torch.load('remaining_dataset.pth')\n",
    "\n",
    "# # Create DataLoaders\n",
    "# val_loader = DataLoader(subset_dataset, batch_size=32, shuffle=False)\n",
    "# test_loader = DataLoader(remaining_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40fd883",
   "metadata": {
    "id": "f40fd883"
   },
   "source": [
    "<H1>IID</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc7a1568",
   "metadata": {
    "id": "fc7a1568"
   },
   "outputs": [],
   "source": [
    "def distribute_dataset_equally(dataset, num_clients):\n",
    "    # Group data by class\n",
    "    class_data = {}\n",
    "    for data, label in dataset:\n",
    "        if label not in class_data:\n",
    "            class_data[label] = []\n",
    "        class_data[label].append((data, label))\n",
    "\n",
    "    # Distribute data\n",
    "    client_data = [[] for _ in range(num_clients)]\n",
    "    for label, data in class_data.items():\n",
    "        data_len = len(data)\n",
    "        base_size = data_len // num_clients\n",
    "        remain = data_len - base_size * num_clients\n",
    "\n",
    "        current_idx = 0\n",
    "        for i in range(num_clients):\n",
    "            end_idx = current_idx + base_size + (1 if i < remain else 0)\n",
    "            client_data[i].extend(data[current_idx:end_idx])\n",
    "            current_idx = end_idx\n",
    "\n",
    "\n",
    "    print_iid_distribution(client_data)\n",
    "    plot_iid_dataset(client_data,num_clients)\n",
    "\n",
    "    return client_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46cb27b0",
   "metadata": {
    "id": "46cb27b0"
   },
   "outputs": [],
   "source": [
    "#print client iid_data distribution\n",
    "def print_iid_distribution(iid_datasets):\n",
    "    # Check if the distribution is correct\n",
    "    for i, client_data in enumerate(iid_datasets):\n",
    "        print(f\"Client {i + 1} data size: {len(client_data)}\")\n",
    "        class_counts = {j: 0 for j in range(10)}\n",
    "        for _, label in client_data:\n",
    "            class_counts[label] += 1\n",
    "        print(f\"Class distribution: {class_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4093ff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot client iid_data distribution\n",
    "def plot_iid_dataset(iid_datasets, num_clients):\n",
    "    # CIFAR-10 class names\n",
    "    class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "    # Calculate fraction of distribution for each class across clients\n",
    "    fractions = []\n",
    "    for client_data in iid_datasets:\n",
    "        class_counts = {j: 0 for j in range(10)}\n",
    "        for _, label in client_data:\n",
    "            class_counts[label] += 1\n",
    "        total_data = len(client_data)\n",
    "        fractions.append([class_counts[i] / total_data for i in range(10)])\n",
    "    fractions = np.array(fractions)\n",
    "    # Define colors for each class\n",
    "    colors = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"#FFA500\", \"#FF00FF\", \"#808080\", \"#00FF00\"]\n",
    "    # Generate the 3D bar chart\n",
    "    fig = plt.figure(figsize=(10, 7))  # Adjust the size here\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    xpos, ypos = np.meshgrid(np.arange(num_clients), np.arange(10), indexing=\"ij\")\n",
    "    for i in range(10):\n",
    "        ax.bar3d(xpos[:, i], ypos[:, i], np.zeros_like(xpos[:, i]),0.75, 0.75, fractions[:, i],shade=True, color=colors[i])\n",
    "\n",
    "    ax.set_xlabel('Client Number',labelpad=5)\n",
    "    ax.set_ylabel('CIFAR-10 Classes',labelpad=15)\n",
    "    ax.set_zlabel('Fraction of Distribution',labelpad=5)\n",
    "    ax.set_xticks(np.arange(0.5, num_clients))\n",
    "    ax.set_xticklabels([str(i+1) for i in range(num_clients)], rotation=45)\n",
    "    ax.set_yticks(np.arange(0.5, 10))\n",
    "    ax.set_yticklabels(class_names, rotation=-60)\n",
    "\n",
    "    #ax.view_init(elev=40, azim=60)\n",
    "    ax.view_init(elev=40, azim=10)\n",
    "\n",
    "    plt.subplots_adjust(left=0.01, right=0.99, bottom=0.01, top=0.99)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef90546",
   "metadata": {
    "id": "bef90546"
   },
   "source": [
    "<H1>non-IID</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f7fa1c5",
   "metadata": {
    "id": "1f7fa1c5"
   },
   "outputs": [],
   "source": [
    "#Print the distribution non IID\n",
    "def print_distribution(client_data):\n",
    "    # Check distribution\n",
    "    for l, client_data_value in enumerate(client_data):\n",
    "        print(f\"Client {l + 1} data size: {len(client_data_value)}\")\n",
    "        class_counts = {j: 0 for j in range(10)}\n",
    "        for _, label in client_data_value:\n",
    "            class_counts[label] += 1\n",
    "        print(f\"Class distribution: {class_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8419b107",
   "metadata": {
    "id": "8419b107"
   },
   "outputs": [],
   "source": [
    "# #Non IID code\n",
    "def distribute_dataset_dirichlet(dataset, num_clients, alpha):\n",
    "    # Group data by class\n",
    "    class_data = {}\n",
    "    for data, label in dataset:\n",
    "        if label not in class_data:\n",
    "            class_data[label] = []\n",
    "        class_data[label].append((data, label))\n",
    "\n",
    "    client_data = [[] for _ in range(num_clients)]\n",
    "    for _, data_list in class_data.items():\n",
    "        # Shuffle data for randomness\n",
    "        np.random.shuffle(data_list)\n",
    "\n",
    "        # Get proportions for data split based on Dirichlet distribution\n",
    "        proportions = np.random.dirichlet([alpha]*num_clients)\n",
    "        # print(\"Proportions: \", proportions)\n",
    "        total_data = len(data_list)\n",
    "        # print(\"total_data: \", total_data)\n",
    "        data_splits = [int(proportions[i]*total_data) for i in range(num_clients)]\n",
    "        # print(\"Data_Split: \", data_splits)\n",
    "\n",
    "        # Adjust the splits to account for rounding errors\n",
    "        # print(\"Before data_split:\",data_splits)\n",
    "        data_splits[-1] += total_data - sum(data_splits)\n",
    "        print(\"After data_split:\",data_splits)\n",
    "\n",
    "        start_idx = 0\n",
    "        for i, split in enumerate(data_splits):\n",
    "            end_idx = start_idx + split\n",
    "            client_data[i].extend(data_list[start_idx:end_idx])\n",
    "            start_idx = end_idx\n",
    "    print(\"Client Number: \",num_clients, \"Alpha: \", alpha)\n",
    "    print_distribution(client_data)\n",
    "    plot_distribution(client_data, num_clients )\n",
    "    return client_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4fa9615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the graphs for nonIID\n",
    "def plot_distribution(client_datasets, num_clients):\n",
    "    # CIFAR-10 class names\n",
    "    class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "    # Calculate fraction of distribution for each class across clients\n",
    "    fractions = []\n",
    "    for client_data in client_datasets:\n",
    "        class_counts = {j: 0 for j in range(10)}\n",
    "        for _, label in client_data:\n",
    "            class_counts[label] += 1\n",
    "        total_data = len(client_data)\n",
    "        # print(total_data)\n",
    "        fractions.append([class_counts[i] / total_data for i in range(10)])\n",
    "    fractions = np.array(fractions)\n",
    "    # Define distinct colors for classes using a colormap\n",
    "    colors = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"#FFA500\", \"#FF00FF\", \"#808080\", \"#00FF00\"]\n",
    "    # # Generate the 3D bar chart\n",
    "    # fig = plt.figure(figsize=(18, 12))\n",
    "    # ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Generate the 3D bar chart\n",
    "    fig = plt.figure(figsize=(10, 7))  # Adjust the size here\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    xpos, ypos = np.meshgrid(np.arange(num_clients), np.arange(10), indexing=\"ij\")\n",
    "\n",
    "    for i in range(10):\n",
    "        ax.bar3d(xpos[:, i], ypos[:, i], np.zeros_like(xpos[:, i]),0.75, 0.75, fractions[:, i],shade=True, color=colors[i])\n",
    "\n",
    "    ax.set_xlabel('Client Number')\n",
    "    ax.set_ylabel('CIFAR-10 Classes')\n",
    "    ax.set_zlabel('Fraction of Distribution')\n",
    "    ax.set_title('Distribution of CIFAR-10 Classes across Clients based on Dirichlet Distribution')\n",
    "    ax.set_xticks(np.arange(0.5, num_clients))\n",
    "    ax.set_xticklabels([str(i+1) for i in range(num_clients)], fontsize=12)\n",
    "    ax.set_yticks(np.arange(0.5, 10))\n",
    "    ax.set_yticklabels(class_names)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f95a6",
   "metadata": {},
   "source": [
    "# Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2700fb3",
   "metadata": {
    "id": "f2700fb3"
   },
   "outputs": [],
   "source": [
    "def accuracy(outp, target):\n",
    "    \"\"\"Computes accuracy\"\"\"\n",
    "    with torch.no_grad():\n",
    "        pred = torch.argmax(outp, dim=1)\n",
    "        correct = pred.eq(target).float().sum().item()\n",
    "        return 100.0 * correct / target.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9306ada",
   "metadata": {
    "id": "b9306ada"
   },
   "outputs": [],
   "source": [
    "def Print(string, dictionary):\n",
    "    first_key = next(iter(dictionary))\n",
    "    first_value = dictionary[first_key]\n",
    "    print(f\"{string}:{first_key}: {first_value[0][0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0db37362",
   "metadata": {
    "id": "0db37362"
   },
   "outputs": [],
   "source": [
    "def forbinus_norm_function(w_i):\n",
    "    value = 0\n",
    "    for k in w_i.keys():\n",
    "        value += torch.linalg.norm(w_i[k])\n",
    "    return value.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e387f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_deviation_function(w_i, w_f):\n",
    "    model_deviation = 0\n",
    "    for k in w_i.keys():\n",
    "        model_deviation += torch.linalg.norm(w_f[k].to(torch.float) - w_i[k].to(torch.float)) / torch.linalg.norm(w_i[k].to(torch.float))\n",
    "    #print(model_deviation.item())\n",
    "    return model_deviation.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53678293",
   "metadata": {
    "id": "53678293"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0a3ffbe",
   "metadata": {
    "id": "e0a3ffbe"
   },
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 10)  # Change output to 10 for CIFAR-10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.relu4(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu5(self.fc1(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7eaf8a",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "833d43d8",
   "metadata": {
    "id": "833d43d8"
   },
   "outputs": [],
   "source": [
    "def train(i_weights, epochs, train_loader, le_rate, cli,roun, epoch_flag):\n",
    "    global opti\n",
    "    local_model = model().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if opti==\"adam\":\n",
    "        optimizer = torch.optim.Adam(local_model.parameters(), lr=le_rate)\n",
    "    elif opti==\"sgd\":\n",
    "        optimizer = torch.optim.SGD(local_model.parameters(), lr=le_rate)\n",
    "    elif opti==\"AdamW\":\n",
    "        optimizer = torch.optim.AdamW(local_model.parameters(), lr=le_rate)\n",
    "    elif opti==\"rmsprop\":\n",
    "        optimizer = torch.optim.RMSprop(local_model.parameters(), lr=le_rate)\n",
    "    elif opti==\"adadelta\":\n",
    "        optimizer = torch.optim.Adadelta(local_model.parameters(), lr=le_rate)\n",
    "    elif opti==\"adagrad\":\n",
    "        optimizer = torch.optim.Adagrad(local_model.parameters(), lr=le_rate)\n",
    "    \n",
    "    epoch_train_accuracy=0 \n",
    "    epoch_train_loss=0\n",
    "    epoch_test_accuracy=0\n",
    "    epoch_test_loss=0\n",
    "    epoch_rmd=0\n",
    "\n",
    "    local_model.load_state_dict(i_weights)\n",
    "\n",
    "    local_model.train()  # Set the model to training mode\n",
    "\n",
    "    # initial weights cathing and printing\n",
    "    initial_weights = {k: v.clone() for k, v in local_model.state_dict().items()}\n",
    "    #Print(\"Model's inside the function Initial weights for client\",initial_weights)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_flag=epoch_flag+1\n",
    "        # gradients_this_epoch = {}\n",
    "        total_samples = 0\n",
    "        total_loss=0\n",
    "        correct_samples = 0\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward + backward + optimize\n",
    "            outputs = local_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(local_model.parameters(), max_norm=1.0)\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = outputs.max(1)  # Get the index of the maximum value in outputs (predicted class)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_samples += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        if(total_samples!=0 and len(train_loader)!=0):\n",
    "            epoch_accuracy = 100 * correct_samples / total_samples\n",
    "            epoch_loss = total_loss / len(train_loader)\n",
    "        else:\n",
    "            epoch_accuracy = 100 * correct_samples / (total_samples+1)\n",
    "            epoch_loss = total_loss / (len(train_loader)+1)\n",
    "        #print(f\"Round {roun}, cleint {cli}, epoch {epoch}: epoch_accuracy {epoch_accuracy}, epoch_loss {epoch_loss} \")\n",
    "    \n",
    "    f_weights = {k: v.clone() for k, v in local_model.state_dict().items()}\n",
    "\n",
    "    #print(f\"\\n Round {roun}, cleint {cli}: epoch_accuracy {epoch_accuracy}, epoch_loss {epoch_loss} \\n\")\n",
    "    epoch_train_accuracy=epoch_accuracy\n",
    "    epoch_train_loss=epoch_loss\n",
    "    epoch_test_accuracy, epoch_test_loss= test(f_weights, test_loader)\n",
    "    \n",
    "    \n",
    "    epoch_rmd=model_deviation_function(initial_weights,f_weights)\n",
    "    \n",
    "    #saving data into dataframe\n",
    "    epoch_data = [epoch_train_accuracy, epoch_train_loss, epoch_test_accuracy, epoch_test_loss, epoch_rmd]\n",
    "    epoch_results.loc[len(epoch_results)] = epoch_data\n",
    "    \n",
    "    return epoch_accuracy,epoch_loss, f_weights, epoch_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b882b03e",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d5cef1c",
   "metadata": {
    "id": "9d5cef1c"
   },
   "outputs": [],
   "source": [
    "def test(w,data):\n",
    "    lmodel = model().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()  # Assuming a classification task\n",
    "    #optimizer = torch.optim.SGD(lmodel.parameters(), lr=learning_rate)\n",
    "    lmodel.load_state_dict(w)\n",
    "    lmodel.eval()\n",
    "\n",
    "    #checking the weights\n",
    "    tw = lmodel.state_dict()\n",
    "    #Print(\"Model's before testing the weights in global model\",tw)\n",
    "\n",
    "    # Evaluation phase for test set\n",
    "    acc_list = []\n",
    "    loss_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j, data in enumerate(data, 0):\n",
    "            images, labels = data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            out = lmodel(images)\n",
    "            # Calculate loss\n",
    "            loss = criterion(out, labels)\n",
    "            loss_list.append(loss.item())\n",
    "            #calculate accuracy\n",
    "            acc = accuracy(out, labels)\n",
    "            acc_list.append(acc)\n",
    "    test_loss = np.mean(loss_list)\n",
    "    test_accuracy = np.mean(acc_list)\n",
    "    #print(\"Model's Test accuracy : {:.2f}%\".format(test_accuracy))\n",
    "    return test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1aae2",
   "metadata": {},
   "source": [
    "# FL Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0d35af3",
   "metadata": {
    "id": "c0d35af3"
   },
   "outputs": [],
   "source": [
    "def federated_learning(i_w, data_client, C, P, R, E, learning_rate, b_size):\n",
    "    \n",
    "    global total_cleints_list, participating_client_list, val_loader\n",
    "    \n",
    "    global_model.load_state_dict(i_w)\n",
    "    #Print(\"Model's initial weights\", i_w)\n",
    "    r_flag=0\n",
    "\n",
    "    #loop for round\n",
    "    for r in range(1,R+1):\n",
    "        round_train_accuracy=0\n",
    "        round_train_loss=0\n",
    "        round_test_accuracy=0\n",
    "        round_test_loss=0\n",
    "        epoch_flag=0\n",
    "\n",
    "        #saving initial weights for spiking model\n",
    "        i_w = {k: v.clone() for k, v in global_model.state_dict().items()}\n",
    "        #Print(\"Model's initial weights\", i_w)\n",
    "        \n",
    "        #colleting weights and results\n",
    "        all_final_weights={}\n",
    "        train_accuracy_list=[]\n",
    "        train_loss_list=[]\n",
    "        \n",
    "        # Randomly select clients\n",
    "        selected_clients = random.sample(total_cleints_list, P)\n",
    "        participating_client_list.append(selected_clients)\n",
    "\n",
    "        #loop for client\n",
    "        for c, data in enumerate(data_client):\n",
    "            \n",
    "            if(c in selected_clients):\n",
    "                \n",
    "                train_loader = torch.utils.data.DataLoader(data, batch_size=b_size, shuffle=True)\n",
    "                \n",
    "                #train model\n",
    "                train_accuracy, train_loss, c_f_weights, epoch_flag = train(i_w, E, train_loader, learning_rate, c, r,epoch_flag)\n",
    "\n",
    "                train_accuracy_list.append(train_accuracy)\n",
    "                train_loss_list.append(train_loss)\n",
    "\n",
    "                # Accumulate weights for the selected client\n",
    "                for param_name, param_grad in c_f_weights.items():\n",
    "                    if param_name in all_final_weights:\n",
    "                        all_final_weights[param_name] += param_grad\n",
    "                    else:\n",
    "                        all_final_weights[param_name] = param_grad\n",
    "\n",
    "            else:\n",
    "                print(f\"client {c} is not selectecd\")\n",
    "        \n",
    "        round_epoch=(epoch_flag)\n",
    "        \n",
    "        #print(\"Total number of selected clients is\", client_counter)\n",
    "        round_train_loss=sum(train_loss_list)/len(train_loss_list)\n",
    "        round_train_accuracy=sum(train_accuracy_list)/len(train_accuracy_list)\n",
    "\n",
    "        print(f\"Model's Round: {r}, train accuracy of model: {round_train_accuracy}, train loss of model: {round_train_loss} \\n\\n\")\n",
    "\n",
    "        for param_name in all_final_weights:\n",
    "            all_final_weights[param_name] = all_final_weights[param_name].float() / len(selected_clients)\n",
    "\n",
    "        round_test_accuracy, round_test_loss=test(all_final_weights, test_loader)\n",
    "        print(f\"Model's Round: {r}, test accuracy of model: {round_test_accuracy}, test loss of model: {round_test_loss} \\n\\n\")\n",
    "        \n",
    "        round_val_accuracy, round_val_loss=test(all_final_weights, val_loader)\n",
    "        print(f\"Model's Round: {r}, val accuracy of model: {round_val_accuracy}, val loss of model: {round_val_loss} \\n\\n\")\n",
    "        \n",
    "        list_accuracy.append(round_train_accuracy)\n",
    "        list_loss.append(round_train_loss)\n",
    "        list_val_accuracy.append(round_val_accuracy)\n",
    "        list_val_loss.append(round_val_loss)\n",
    "        list_test_accuracy.append(round_test_accuracy)\n",
    "        list_test_loss.append(round_test_loss)\n",
    "        \n",
    "        #model deviation code\n",
    "        round_rmd=model_deviation_function(i_w, all_final_weights)\n",
    "        #print(\"Model deviation values: \", model_deviation)\n",
    "\n",
    "        #saving data into dataframe\n",
    "        round_data = [round_train_accuracy, round_train_loss, round_test_accuracy, round_test_loss, round_rmd, round_epoch, round_val_accuracy, round_val_loss]\n",
    "        round_results.loc[len(round_results)] = round_data\n",
    "        \n",
    "        # if (list_val_loss[r] > list_val_loss[r-1]) and (list_loss[r] > list_loss[r-1]): \n",
    "        #     r_flag=r_flag+1\n",
    "        #     if r_flag==3:\n",
    "        #         print(\"Early stopping of rounds at round no : \",r,\"\\n\\n\")\n",
    "        #         break\n",
    "            \n",
    "        global_model.load_state_dict(all_final_weights)\n",
    "        print(\"round\", r, \"completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512e1d4",
   "metadata": {
    "id": "8512e1d4"
   },
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50345a9f",
   "metadata": {
    "id": "50345a9f"
   },
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fda6c6a",
   "metadata": {
    "id": "7fda6c6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's initial weights:conv1.weight: tensor([[-0.0619,  0.0628, -0.1613],\n",
      "        [-0.1879,  0.0229,  0.1701],\n",
      "        [-0.1736, -0.0053,  0.0203]], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#===========================Parameters==============================================================\n",
    "client_no=20\n",
    "participating_client=20\n",
    "epochs=5\n",
    "learning_rate=0.01\n",
    "round_no=30\n",
    "batch_size=128\n",
    "data_class=10\n",
    "\n",
    "# distributions = \"non_iid\" # 'non_iid'\n",
    "distributions = \"non_iid\" # 'non_iid'\n",
    "\n",
    "\n",
    "# alpha=\"infinity\"\n",
    "alpha=0.5\n",
    "# alpha=0.2\n",
    "\n",
    "# opti=\"sgd\"\n",
    "# opti=\"AdamW\"\n",
    "opti=\"adam\"\n",
    "# opti=\"adagrad\"\n",
    "# opti=\"rmsprop\"\n",
    "# opti=\"adadelta\"\n",
    "\n",
    "\n",
    "# List of clients\n",
    "total_cleints_list = list(range(0, client_no))\n",
    "# print(total_cleints_list)\n",
    "participating_client_list=[]\n",
    "\n",
    "# Define dataframe for round results\n",
    "round_columns = ['train_accuracy', 'train_loss', 'test_accuracy', 'test_loss', 'rmd', 'epoch', 'val_accuracy', 'val_loss']\n",
    "round_results = pd.DataFrame(columns=round_columns)\n",
    "\n",
    "# Define dataframe for epoch results\n",
    "epoch_columns = ['train_accuracy', 'train_loss', 'test_accuracy', 'test_loss', 'rmd']\n",
    "epoch_results = pd.DataFrame(columns=epoch_columns)\n",
    "\n",
    "list_accuracy=[]\n",
    "list_loss=[]\n",
    "list_test_accuracy=[]\n",
    "list_test_loss=[]\n",
    "list_val_accuracy=[]\n",
    "list_val_loss=[]\n",
    "\n",
    "#===================================loading the saved weight list====================================================\n",
    "global_model = model().to(device)\n",
    "# initial_weights={k: v.clone() for k, v in global_model.state_dict().items()}\n",
    "# Save the initial weights\n",
    "file_path= \"s_cnn.pth\"\n",
    "# torch.save(initial_weights, file_path)\n",
    "initial_weights=torch.load(file_path,weights_only=False)\n",
    "Print(\"Model's initial weights\", initial_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a055f33a",
   "metadata": {},
   "source": [
    "<H1>Divide data among cleints</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3f8ad92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #=================================loading IID data===========================\n",
    "# if distributions == 'iid':\n",
    "#     client_datasets = distribute_dataset_equally(train_dataset,client_no)\n",
    "    \n",
    "# elif distributions == 'non_iid':\n",
    "#     client_datasets = distribute_dataset_dirichlet(train_dataset, client_no, alpha)\n",
    "# else:\n",
    "#     print(\"provide a valid distribution Please\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0549cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save client_datasets to a file\n",
    "# if distributions == 'iid':\n",
    "#     with open('client_datasets_IID.pkl', 'wb') as f:\n",
    "#         pickle.dump(client_datasets, f)\n",
    "# elif distributions == 'non_iid' and alpha==0.5:\n",
    "#     with open('client_datasets_non_IID_0_5.pkl', 'wb') as f:\n",
    "#         pickle.dump(client_datasets, f)\n",
    "# elif distributions == 'non_iid' and alpha==0.2:\n",
    "#     with open('client_datasets_non_IID_0_2.pkl', 'wb') as f:\n",
    "#         pickle.dump(client_datasets, f)\n",
    "\n",
    "# print(\"client_datasets saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aef0b71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_datasets loaded successfully.\n",
      "Client 1 data size: 1928\n",
      "Class distribution: {0: 93, 1: 405, 2: 144, 3: 3, 4: 342, 5: 365, 6: 25, 7: 64, 8: 248, 9: 239}\n",
      "Client 2 data size: 1997\n",
      "Class distribution: {0: 38, 1: 0, 2: 921, 3: 588, 4: 14, 5: 9, 6: 58, 7: 51, 8: 11, 9: 307}\n",
      "Client 3 data size: 3557\n",
      "Class distribution: {0: 849, 1: 41, 2: 2, 3: 208, 4: 645, 5: 640, 6: 853, 7: 151, 8: 20, 9: 148}\n",
      "Client 4 data size: 1954\n",
      "Class distribution: {0: 369, 1: 91, 2: 9, 3: 135, 4: 108, 5: 65, 6: 267, 7: 30, 8: 673, 9: 207}\n",
      "Client 5 data size: 3332\n",
      "Class distribution: {0: 25, 1: 0, 2: 866, 3: 1163, 4: 6, 5: 194, 6: 13, 7: 375, 8: 61, 9: 629}\n",
      "Client 6 data size: 2673\n",
      "Class distribution: {0: 241, 1: 643, 2: 161, 3: 26, 4: 2, 5: 861, 6: 65, 7: 182, 8: 462, 9: 30}\n",
      "Client 7 data size: 2750\n",
      "Class distribution: {0: 161, 1: 0, 2: 503, 3: 26, 4: 184, 5: 443, 6: 6, 7: 6, 8: 1412, 9: 9}\n",
      "Client 8 data size: 2071\n",
      "Class distribution: {0: 217, 1: 128, 2: 136, 3: 37, 4: 20, 5: 19, 6: 42, 7: 371, 8: 37, 9: 1064}\n",
      "Client 9 data size: 2185\n",
      "Class distribution: {0: 0, 1: 286, 2: 426, 3: 441, 4: 223, 5: 651, 6: 5, 7: 23, 8: 107, 9: 23}\n",
      "Client 10 data size: 1544\n",
      "Class distribution: {0: 602, 1: 10, 2: 2, 3: 16, 4: 91, 5: 47, 6: 299, 7: 192, 8: 255, 9: 30}\n",
      "Client 11 data size: 681\n",
      "Class distribution: {0: 2, 1: 43, 2: 1, 3: 75, 4: 26, 5: 75, 6: 101, 7: 1, 8: 205, 9: 152}\n",
      "Client 12 data size: 1524\n",
      "Class distribution: {0: 10, 1: 2, 2: 79, 3: 142, 4: 1, 5: 6, 6: 670, 7: 90, 8: 1, 9: 523}\n",
      "Client 13 data size: 1346\n",
      "Class distribution: {0: 281, 1: 82, 2: 8, 3: 208, 4: 116, 5: 206, 6: 5, 7: 128, 8: 158, 9: 154}\n",
      "Client 14 data size: 4708\n",
      "Class distribution: {0: 596, 1: 249, 2: 1220, 3: 246, 4: 932, 5: 595, 6: 18, 7: 717, 8: 33, 9: 102}\n",
      "Client 15 data size: 1893\n",
      "Class distribution: {0: 30, 1: 9, 2: 65, 3: 842, 4: 145, 5: 0, 6: 605, 7: 34, 8: 64, 9: 99}\n",
      "Client 16 data size: 2479\n",
      "Class distribution: {0: 0, 1: 924, 2: 71, 3: 176, 4: 10, 5: 259, 6: 54, 7: 897, 8: 0, 9: 88}\n",
      "Client 17 data size: 2031\n",
      "Class distribution: {0: 78, 1: 165, 2: 14, 3: 412, 4: 10, 5: 397, 6: 69, 7: 1, 8: 643, 9: 242}\n",
      "Client 18 data size: 6129\n",
      "Class distribution: {0: 673, 1: 1721, 2: 41, 3: 127, 4: 1129, 5: 118, 6: 29, 7: 1383, 8: 100, 9: 808}\n",
      "Client 19 data size: 1348\n",
      "Class distribution: {0: 70, 1: 153, 2: 1, 3: 96, 4: 46, 5: 35, 6: 541, 7: 159, 8: 139, 9: 108}\n",
      "Client 20 data size: 3870\n",
      "Class distribution: {0: 665, 1: 48, 2: 330, 3: 33, 4: 950, 5: 15, 6: 1275, 7: 145, 8: 371, 9: 38}\n"
     ]
    }
   ],
   "source": [
    "# Load client_datasets from a file\n",
    "if distributions == 'iid':\n",
    "    with open('20_client_datasets_IID.pkl', 'rb') as f:\n",
    "        client_datasets = pickle.load(f)\n",
    "\n",
    "elif distributions == 'non_iid' and alpha==0.5:\n",
    "    with open('20_client_datasets_non_IID_0_5.pkl', 'rb') as f:\n",
    "        client_datasets = pickle.load(f)\n",
    "    \n",
    "elif distributions == 'non_iid' and alpha==0.2:\n",
    "    with open('20_client_datasets_non_IID_0_2.pkl', 'rb') as f:\n",
    "        client_datasets = pickle.load(f)\n",
    "        \n",
    "print(\"client_datasets loaded successfully.\")\n",
    "print_distribution(client_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec69f6d-2802-4aff-8fdc-d8371495125a",
   "metadata": {},
   "source": [
    "<H1>Round zero</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "516b645b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.039893617021276, 2.3027353875180507, 9.889240506329115, 2.3031224721594703  \n",
      "initial_weights:conv1.weight: tensor([[-0.0619,  0.0628, -0.1613],\n",
      "        [-0.1879,  0.0229,  0.1701],\n",
      "        [-0.1736, -0.0053,  0.0203]], device='cuda:0')\n",
      "\n",
      " train accuracy: 11.262635182370797\n",
      " train_loss: 2.300176019249503\n",
      " test_accuracy: 10.039893617021276\n",
      " test_loss: 2.3027353875180507\n"
     ]
    }
   ],
   "source": [
    "#train accuracy for cleints\n",
    "round_train_accuracy=0\n",
    "round_train_loss=0\n",
    "train_accuracy_list=[]\n",
    "train_loss_list=[]\n",
    "for c, data in enumerate(client_datasets):\n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "    train_accuracy, train_loss=test(initial_weights, train_loader)\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "    train_loss_list.append(train_loss)\n",
    "round_train_accuracy=(sum(train_accuracy_list)/len(train_accuracy_list))\n",
    "round_train_loss=(sum(train_loss_list)/len(train_loss_list))\n",
    "\n",
    "\n",
    "list_accuracy.append(round_train_accuracy)\n",
    "list_loss.append(round_train_loss)\n",
    "\n",
    "#test accuracy for server\n",
    "round_test_accuracy=0\n",
    "round_test_loss=0\n",
    "test_accuracy,test_loss=test(initial_weights,test_loader)\n",
    "val_accuracy,val_loss=test(initial_weights,val_loader)\n",
    "\n",
    "print(f\"{test_accuracy}, {test_loss}, {val_accuracy}, {val_loss}  \")\n",
    "round_test_accuracy=(test_accuracy)\n",
    "round_test_loss=(test_loss)\n",
    "\n",
    "round_rmd=0\n",
    "round_epoch=0\n",
    "\n",
    "round_data = [round_train_accuracy, round_train_loss, round_test_accuracy, round_test_loss, round_rmd, round_epoch, val_accuracy,val_loss]\n",
    "round_results.loc[len(round_results)] = round_data\n",
    "\n",
    "list_val_accuracy.append(val_accuracy)\n",
    "list_val_loss.append(val_loss)\n",
    "list_test_accuracy.append(test_accuracy)\n",
    "list_test_loss.append(test_loss)\n",
    "\n",
    "\n",
    "Print(\"initial_weights\", initial_weights)\n",
    "print(f' train accuracy: {round_train_accuracy}\\n train_loss: {round_train_loss}\\n test_accuracy: {round_test_accuracy}\\n test_loss: {round_test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c3444a",
   "metadata": {},
   "source": [
    "<H1>Run FL</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "NJ_CpP0b9OUo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJ_CpP0b9OUo",
    "outputId": "961d3fb1-5fee-4497-be83-fff67f5e10e3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's Round: 1, train accuracy of model: 43.64303862882131, train loss of model: 1.5229489655425121 \n",
      "\n",
      "\n",
      "Model's Round: 1, test accuracy of model: 10.039893617021276, test loss of model: 2.3033408073668786 \n",
      "\n",
      "\n",
      "Model's Round: 1, val accuracy of model: 9.889240506329115, val loss of model: 2.304065508178518 \n",
      "\n",
      "\n",
      "round 1 completed\n",
      "Model's Round: 2, train accuracy of model: 34.83426391673835, train loss of model: 1.7139750044481592 \n",
      "\n",
      "\n",
      "Model's Round: 2, test accuracy of model: 9.97340425531915, test loss of model: 2.3174522410047813 \n",
      "\n",
      "\n",
      "Model's Round: 2, val accuracy of model: 9.889240506329115, val loss of model: 2.3176073273525963 \n",
      "\n",
      "\n",
      "round 2 completed\n",
      "Model's Round: 3, train accuracy of model: 34.572635112166964, train loss of model: 1.7145793646129845 \n",
      "\n",
      "\n",
      "Model's Round: 3, test accuracy of model: 9.97340425531915, test loss of model: 2.3192957543312236 \n",
      "\n",
      "\n",
      "Model's Round: 3, val accuracy of model: 9.889240506329115, val loss of model: 2.319249135029467 \n",
      "\n",
      "\n",
      "round 3 completed\n",
      "Model's Round: 4, train accuracy of model: 34.85722902321334, train loss of model: 1.7126571300537397 \n",
      "\n",
      "\n",
      "Model's Round: 4, test accuracy of model: 9.97340425531915, test loss of model: 2.319980821203678 \n",
      "\n",
      "\n",
      "Model's Round: 4, val accuracy of model: 9.889240506329115, val loss of model: 2.3199126358273663 \n",
      "\n",
      "\n",
      "round 4 completed\n",
      "Model's Round: 5, train accuracy of model: 34.82795718383829, train loss of model: 1.7113917254033968 \n",
      "\n",
      "\n",
      "Model's Round: 5, test accuracy of model: 9.97340425531915, test loss of model: 2.31805216302263 \n",
      "\n",
      "\n",
      "Model's Round: 5, val accuracy of model: 9.889240506329115, val loss of model: 2.3178140422965905 \n",
      "\n",
      "\n",
      "round 5 completed\n",
      "Model's Round: 6, train accuracy of model: 34.797168587013154, train loss of model: 1.7118236574926624 \n",
      "\n",
      "\n",
      "Model's Round: 6, test accuracy of model: 9.97340425531915, test loss of model: 2.318592727945206 \n",
      "\n",
      "\n",
      "Model's Round: 6, val accuracy of model: 9.889240506329115, val loss of model: 2.3183244542230534 \n",
      "\n",
      "\n",
      "round 6 completed\n",
      "Model's Round: 7, train accuracy of model: 34.75559301001521, train loss of model: 1.7160962809916076 \n",
      "\n",
      "\n",
      "Model's Round: 7, test accuracy of model: 9.97340425531915, test loss of model: 2.3170386649192647 \n",
      "\n",
      "\n",
      "Model's Round: 7, val accuracy of model: 9.889240506329115, val loss of model: 2.316897313806075 \n",
      "\n",
      "\n",
      "round 7 completed\n",
      "Model's Round: 8, train accuracy of model: 34.75807008104832, train loss of model: 1.7154216094454948 \n",
      "\n",
      "\n",
      "Model's Round: 8, test accuracy of model: 9.97340425531915, test loss of model: 2.317051901715867 \n",
      "\n",
      "\n",
      "Model's Round: 8, val accuracy of model: 9.889240506329115, val loss of model: 2.3167803468583505 \n",
      "\n",
      "\n",
      "round 8 completed\n",
      "Model's Round: 9, train accuracy of model: 34.647794183397224, train loss of model: 1.71204585699699 \n",
      "\n",
      "\n",
      "Model's Round: 9, test accuracy of model: 9.97340425531915, test loss of model: 2.3177259485772317 \n",
      "\n",
      "\n",
      "Model's Round: 9, val accuracy of model: 9.889240506329115, val loss of model: 2.317658219156386 \n",
      "\n",
      "\n",
      "round 9 completed\n",
      "Model's Round: 10, train accuracy of model: 34.58903630477073, train loss of model: 1.7133846496858702 \n",
      "\n",
      "\n",
      "Model's Round: 10, test accuracy of model: 9.97340425531915, test loss of model: 2.319312445660855 \n",
      "\n",
      "\n",
      "Model's Round: 10, val accuracy of model: 9.889240506329115, val loss of model: 2.3189588196669955 \n",
      "\n",
      "\n",
      "round 10 completed\n",
      "Model's Round: 11, train accuracy of model: 34.735232692528015, train loss of model: 1.7131466427913153 \n",
      "\n",
      "\n",
      "Model's Round: 11, test accuracy of model: 9.97340425531915, test loss of model: 2.3193963253751715 \n",
      "\n",
      "\n",
      "Model's Round: 11, val accuracy of model: 9.889240506329115, val loss of model: 2.3187529256072223 \n",
      "\n",
      "\n",
      "round 11 completed\n",
      "Model's Round: 12, train accuracy of model: 34.61511857409776, train loss of model: 1.7125674932229675 \n",
      "\n",
      "\n",
      "Model's Round: 12, test accuracy of model: 9.97340425531915, test loss of model: 2.3193764169165427 \n",
      "\n",
      "\n",
      "Model's Round: 12, val accuracy of model: 9.889240506329115, val loss of model: 2.319003726862654 \n",
      "\n",
      "\n",
      "round 12 completed\n",
      "Model's Round: 13, train accuracy of model: 34.79561460630887, train loss of model: 1.71350788473698 \n",
      "\n",
      "\n",
      "Model's Round: 13, test accuracy of model: 9.97340425531915, test loss of model: 2.3186544529935147 \n",
      "\n",
      "\n",
      "Model's Round: 13, val accuracy of model: 9.889240506329115, val loss of model: 2.3182330735122103 \n",
      "\n",
      "\n",
      "round 13 completed\n",
      "Model's Round: 14, train accuracy of model: 34.349350360496786, train loss of model: 1.7150032973851097 \n",
      "\n",
      "\n",
      "Model's Round: 14, test accuracy of model: 9.97340425531915, test loss of model: 2.3198266901868454 \n",
      "\n",
      "\n",
      "Model's Round: 14, val accuracy of model: 9.889240506329115, val loss of model: 2.319076664840119 \n",
      "\n",
      "\n",
      "round 14 completed\n",
      "Model's Round: 15, train accuracy of model: 34.9220583822451, train loss of model: 1.7137341643212554 \n",
      "\n",
      "\n",
      "Model's Round: 15, test accuracy of model: 9.97340425531915, test loss of model: 2.320248667737271 \n",
      "\n",
      "\n",
      "Model's Round: 15, val accuracy of model: 9.889240506329115, val loss of model: 2.31961728953108 \n",
      "\n",
      "\n",
      "round 15 completed\n",
      "Model's Round: 16, train accuracy of model: 34.76152339923521, train loss of model: 1.7137929381106676 \n",
      "\n",
      "\n",
      "Model's Round: 16, test accuracy of model: 9.97340425531915, test loss of model: 2.320481551961696 \n",
      "\n",
      "\n",
      "Model's Round: 16, val accuracy of model: 9.889240506329115, val loss of model: 2.3195610378361957 \n",
      "\n",
      "\n",
      "round 16 completed\n",
      "Model's Round: 17, train accuracy of model: 34.529852513720854, train loss of model: 1.7138149305290546 \n",
      "\n",
      "\n",
      "Model's Round: 17, test accuracy of model: 9.97340425531915, test loss of model: 2.320842449715797 \n",
      "\n",
      "\n",
      "Model's Round: 17, val accuracy of model: 9.889240506329115, val loss of model: 2.320198689834981 \n",
      "\n",
      "\n",
      "round 17 completed\n",
      "Model's Round: 18, train accuracy of model: 34.74286075993159, train loss of model: 1.7124527382757737 \n",
      "\n",
      "\n",
      "Model's Round: 18, test accuracy of model: 9.97340425531915, test loss of model: 2.3199149831812433 \n",
      "\n",
      "\n",
      "Model's Round: 18, val accuracy of model: 9.889240506329115, val loss of model: 2.3191759495795528 \n",
      "\n",
      "\n",
      "round 18 completed\n",
      "Model's Round: 19, train accuracy of model: 34.69966707247058, train loss of model: 1.711542677526166 \n",
      "\n",
      "\n",
      "Model's Round: 19, test accuracy of model: 9.97340425531915, test loss of model: 2.319915031879506 \n",
      "\n",
      "\n",
      "Model's Round: 19, val accuracy of model: 9.889240506329115, val loss of model: 2.3191754817962646 \n",
      "\n",
      "\n",
      "round 19 completed\n",
      "Model's Round: 20, train accuracy of model: 34.57509290539317, train loss of model: 1.713048888027909 \n",
      "\n",
      "\n",
      "Model's Round: 20, test accuracy of model: 9.97340425531915, test loss of model: 2.321029667144126 \n",
      "\n",
      "\n",
      "Model's Round: 20, val accuracy of model: 9.889240506329115, val loss of model: 2.3201200086859206 \n",
      "\n",
      "\n",
      "round 20 completed\n",
      "Model's Round: 21, train accuracy of model: 34.44999451028559, train loss of model: 1.7153020325433996 \n",
      "\n",
      "\n",
      "Model's Round: 21, test accuracy of model: 9.97340425531915, test loss of model: 2.3188970880305515 \n",
      "\n",
      "\n",
      "Model's Round: 21, val accuracy of model: 9.889240506329115, val loss of model: 2.318215448645097 \n",
      "\n",
      "\n",
      "round 21 completed\n",
      "Model's Round: 22, train accuracy of model: 34.66019567774849, train loss of model: 1.7156076581733353 \n",
      "\n",
      "\n",
      "Model's Round: 22, test accuracy of model: 9.97340425531915, test loss of model: 2.320514777366151 \n",
      "\n",
      "\n",
      "Model's Round: 22, val accuracy of model: 9.889240506329115, val loss of model: 2.3197179564946815 \n",
      "\n",
      "\n",
      "round 22 completed\n",
      "Model's Round: 23, train accuracy of model: 34.60369436656021, train loss of model: 1.7147360734475587 \n",
      "\n",
      "\n",
      "Model's Round: 23, test accuracy of model: 9.97340425531915, test loss of model: 2.319769153189152 \n",
      "\n",
      "\n",
      "Model's Round: 23, val accuracy of model: 9.889240506329115, val loss of model: 2.318502613260776 \n",
      "\n",
      "\n",
      "round 23 completed\n",
      "Model's Round: 24, train accuracy of model: 34.41439008140466, train loss of model: 1.716041637267007 \n",
      "\n",
      "\n",
      "Model's Round: 24, test accuracy of model: 9.97340425531915, test loss of model: 2.3211845945804677 \n",
      "\n",
      "\n",
      "Model's Round: 24, val accuracy of model: 9.889240506329115, val loss of model: 2.320217464543596 \n",
      "\n",
      "\n",
      "round 24 completed\n",
      "Model's Round: 25, train accuracy of model: 34.70151387354362, train loss of model: 1.713093381665684 \n",
      "\n",
      "\n",
      "Model's Round: 25, test accuracy of model: 9.97340425531915, test loss of model: 2.3218479095621314 \n",
      "\n",
      "\n",
      "Model's Round: 25, val accuracy of model: 9.889240506329115, val loss of model: 2.321109159083306 \n",
      "\n",
      "\n",
      "round 25 completed\n",
      "Model's Round: 26, train accuracy of model: 34.52857280456347, train loss of model: 1.7161414986647217 \n",
      "\n",
      "\n",
      "Model's Round: 26, test accuracy of model: 9.97340425531915, test loss of model: 2.3195684189492085 \n",
      "\n",
      "\n",
      "Model's Round: 26, val accuracy of model: 9.889240506329115, val loss of model: 2.3185611646386644 \n",
      "\n",
      "\n",
      "round 26 completed\n",
      "Model's Round: 27, train accuracy of model: 34.52591396793718, train loss of model: 1.7115344125843162 \n",
      "\n",
      "\n",
      "Model's Round: 27, test accuracy of model: 9.97340425531915, test loss of model: 2.3203759852876056 \n",
      "\n",
      "\n",
      "Model's Round: 27, val accuracy of model: 9.889240506329115, val loss of model: 2.319192050378534 \n",
      "\n",
      "\n",
      "round 27 completed\n",
      "Model's Round: 28, train accuracy of model: 34.37872499891208, train loss of model: 1.7110079350774374 \n",
      "\n",
      "\n",
      "Model's Round: 28, test accuracy of model: 9.97340425531915, test loss of model: 2.3206835168473265 \n",
      "\n",
      "\n",
      "Model's Round: 28, val accuracy of model: 9.889240506329115, val loss of model: 2.3198334204999704 \n",
      "\n",
      "\n",
      "round 28 completed\n",
      "Model's Round: 29, train accuracy of model: 34.684919547742666, train loss of model: 1.7120679671241537 \n",
      "\n",
      "\n",
      "Model's Round: 29, test accuracy of model: 9.97340425531915, test loss of model: 2.3206079198959024 \n",
      "\n",
      "\n",
      "Model's Round: 29, val accuracy of model: 9.889240506329115, val loss of model: 2.3195738369905494 \n",
      "\n",
      "\n",
      "round 29 completed\n",
      "Model's Round: 30, train accuracy of model: 34.812244766233896, train loss of model: 1.7146224058347985 \n",
      "\n",
      "\n",
      "Model's Round: 30, test accuracy of model: 9.97340425531915, test loss of model: 2.3215648265595132 \n",
      "\n",
      "\n",
      "Model's Round: 30, val accuracy of model: 9.889240506329115, val loss of model: 2.3202247710167607 \n",
      "\n",
      "\n",
      "round 30 completed\n"
     ]
    }
   ],
   "source": [
    "federated_learning(initial_weights, client_datasets, client_no, participating_client, round_no, epochs, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a91d029",
   "metadata": {},
   "source": [
    "# Reuslts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6fa4db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully written for round results.\n"
     ]
    }
   ],
   "source": [
    "# 778911 rmd: 0\n",
    "# Round # Define the folder and file name\n",
    "folder_name = f\"fed_avg_{opti}_{learning_rate}_{participating_client}_{client_no}_{distributions}_{alpha}\"  # Folder where the Excel file will be saved\n",
    "file_name = \"round_results.xlsx\"\n",
    "\n",
    "\n",
    "# Check if the folder exists, if not, create it\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Full path where the Excel file will be saved\n",
    "file_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "round_results.to_excel(file_path, index=False)\n",
    "\n",
    "print(\"DataFrame successfully written for round results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcc1e6ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully written for epoch results.\n"
     ]
    }
   ],
   "source": [
    "# Define the folder and file name\n",
    "folder_name =  f\"fed_avg_{opti}_{learning_rate}_{participating_client}_{client_no}_{distributions}_{alpha}\"   # Folder where the Excel file will be saved\n",
    "file_name = \"epoch_results.xlsx\"\n",
    "\n",
    "# Check if the folder exists, if not, create it\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Full path where the Excel file will be saved\n",
    "file_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "epoch_results.to_excel(file_path, index=False)\n",
    "\n",
    "print(\"DataFrame successfully written for epoch results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bccc3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56046b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7a26882b",
    "f40fd883",
    "bef90546"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
